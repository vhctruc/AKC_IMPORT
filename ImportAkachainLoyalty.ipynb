{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import polars as pl\n",
    "import sqlalchemy as sa\n",
    "import urllib\n",
    "from datetime import datetime, timedelta\n",
    "import math\n",
    "import numpy as np\n",
    "import datetime as dt\n",
    "import glob\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# AKC LOGIN:\n",
    "## User: operator_ict\n",
    "## Passwork: Ict@123456"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Note\n",
    "# Ngày 6/3 đổi tỷ lệ điểm từ 40,000VND/Điểm -> 4,000VND/Điểm\n",
    "# Burning thì hoàn toàn là data mớig\n",
    "# Partner Refund thì tự net-off với phiếu song son\n",
    "# Invoice OK đã lấy theo điểm mới\n",
    "#"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getEngine(engine_name):\n",
    "    connection = f\"mssql+pyodbc:///?odbc_connect={urllib.parse.quote_plus(engine_name)}\"\n",
    "    engine = sa.create_engine(connection, fast_executemany=True)\n",
    "    return engine\n",
    "    \n",
    "### Connect to DataMart Server\n",
    "mart_server = '118.69.201.34'\n",
    "mart_database = 'FLC_SHOP_SALES_DATAWAREHOUSE'\n",
    "mart_username = 'ecom_user'\n",
    "mart_password = 'Ec0m@12345'\n",
    "SaleMart = ('DRIVER={ODBC Driver 17 for SQL Server}'\n",
    "            ';SERVER=' + mart_server +\n",
    "            ';DATABASE=' + mart_database +\n",
    "            ';UID=' + mart_username +\n",
    "            ';PWD=' + mart_password)\n",
    "\n",
    "### Connect to Azure server\n",
    "azr_server = \"ssd-synapse-prod.sql.azuresynapse.net\"\n",
    "azr_database = 'frtssdsqlpoolprod'\n",
    "azr_username = 'frtcid'\n",
    "azr_password = 'Frt2022Cid@#123'\n",
    "        \n",
    "connection_url = sa.engine.URL.create(\n",
    "    \"mssql+pyodbc\",\n",
    "    username=azr_username,\n",
    "    password=azr_password,\n",
    "    host=azr_server,\n",
    "    database=azr_database,\n",
    "    query={\n",
    "        \"driver\": \"ODBC Driver 17 for SQL Server\",\n",
    "        \"autocommit\": \"True\",\n",
    "    },\n",
    ")\n",
    "\n",
    "azr_engine = sa.create_engine(connection_url)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Mã thành viên</th>\n",
       "      <th>Ngày tạo</th>\n",
       "      <th>Ngày giao dịch</th>\n",
       "      <th>Lý do</th>\n",
       "      <th>Mã giao dịch</th>\n",
       "      <th>Số tiền mua</th>\n",
       "      <th>Điểm xếp hạng quy đổi</th>\n",
       "      <th>Điểm phát sinh</th>\n",
       "      <th>Điểm đổi quà đã sử dụng</th>\n",
       "      <th>TIME_INSERT</th>\n",
       "      <th>Source</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2204</th>\n",
       "      <td>2248aa17-02d8-487c-b66c-5fef8b2dceb2</td>\n",
       "      <td>2023-06-29</td>\n",
       "      <td>2023-06-29</td>\n",
       "      <td>Invoice</td>\n",
       "      <td>45294445</td>\n",
       "      <td>2990000.0</td>\n",
       "      <td>747</td>\n",
       "      <td>747</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2023-06-30 09:17:44.730910</td>\n",
       "      <td>Data_20230629_20230629</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5758</th>\n",
       "      <td>b7ec2041-8c87-4cf4-b15e-f0656301473f</td>\n",
       "      <td>2023-06-29</td>\n",
       "      <td>2023-06-29</td>\n",
       "      <td>Invoice</td>\n",
       "      <td>45289920</td>\n",
       "      <td>18690000.0</td>\n",
       "      <td>4672</td>\n",
       "      <td>4672</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2023-06-30 09:17:44.730910</td>\n",
       "      <td>Data_20230629_20230629</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1021</th>\n",
       "      <td>109df0fb-0675-48e3-82f3-ec3ffa34a347</td>\n",
       "      <td>2023-06-29</td>\n",
       "      <td>2023-06-29</td>\n",
       "      <td>Invoice</td>\n",
       "      <td>45296032</td>\n",
       "      <td>9000.0</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2023-06-30 09:17:44.730910</td>\n",
       "      <td>Data_20230629_20230629</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                             Mã thành viên   Ngày tạo Ngày giao dịch    Lý do  \\\n",
       "2204  2248aa17-02d8-487c-b66c-5fef8b2dceb2 2023-06-29     2023-06-29  Invoice   \n",
       "5758  b7ec2041-8c87-4cf4-b15e-f0656301473f 2023-06-29     2023-06-29  Invoice   \n",
       "1021  109df0fb-0675-48e3-82f3-ec3ffa34a347 2023-06-29     2023-06-29  Invoice   \n",
       "\n",
       "     Mã giao dịch  Số tiền mua  Điểm xếp hạng quy đổi  Điểm phát sinh  \\\n",
       "2204     45294445    2990000.0                    747             747   \n",
       "5758     45289920   18690000.0                   4672            4672   \n",
       "1021     45296032       9000.0                      2               2   \n",
       "\n",
       "      Điểm đổi quà đã sử dụng                TIME_INSERT  \\\n",
       "2204                      NaN 2023-06-30 09:17:44.730910   \n",
       "5758                      NaN 2023-06-30 09:17:44.730910   \n",
       "1021                      NaN 2023-06-30 09:17:44.730910   \n",
       "\n",
       "                      Source  \n",
       "2204  Data_20230629_20230629  \n",
       "5758  Data_20230629_20230629  \n",
       "1021  Data_20230629_20230629  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "source_ = 'Data_20230629_20230629'\n",
    "df = pd.read_excel(r\"D:/IMPORT_AKC/2023-06/\" + source_ + \".xlsx\")\n",
    "\n",
    "df['Ngày tạo']          = pd.to_datetime(df['Ngày tạo'])\n",
    "df['Ngày giao dịch']    = pd.to_datetime(df['Ngày giao dịch'])\n",
    "\n",
    "col_ = ['Mã thành viên', 'Ngày tạo', 'Ngày giao dịch', 'Lý do', 'Mã giao dịch',\n",
    "       'Số tiền mua', 'Điểm xếp hạng quy đổi', 'Điểm phát sinh',\n",
    "       'Điểm đổi quà đã sử dụng']\n",
    "df = df[col_]\n",
    "df['TIME_INSERT']   = datetime.now()\n",
    "df['Source']        = source_\n",
    "min_date_filter_    = df['Ngày tạo'].min() - timedelta(days=5)\n",
    "df.sample(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of records = 8,303\n",
      "|_1_8,303_100%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "E:\\Anaconda\\lib\\site-packages\\pandas\\io\\sql.py:1760: SAWarning: Could not fetch transaction isolation level, tried views: ('sys.dm_exec_sessions', 'sys.dm_pdw_nodes_exec_sessions'); final error was: ('42000', '[42000] [Microsoft][ODBC Driver 17 for SQL Server][SQL Server]User does not have permission to perform this action. (6004) (SQLExecDirectW)')\n",
      "  insp = sqlalchemy_inspect(self.connectable)\n"
     ]
    }
   ],
   "source": [
    "## Lấy mã giao dịch dò trong Server Azure để tìm ra mã khách hàng\n",
    "\n",
    "list_inv = df[df['Lý do']=='Invoice']['Mã giao dịch'].drop_duplicates()\n",
    "num_part = math.ceil(len(list_inv) /15_000)\n",
    "\n",
    "list_cus_inv_out = pd.DataFrame()\n",
    "print(f\"Number of records = {len(list_inv):,.0f}\")\n",
    "for i in range(1,num_part+1):\n",
    "    list_ = list_inv[(i-1)*15_000 : min(i * 15_000, len(list_inv))]\n",
    "    num_record = min(i * 15_000, len(list_inv))\n",
    "    print(f\"|_{i}_{num_record:,.0f}_{num_record/len(list_inv):,.0%}\")\n",
    "    list_ = ','.join(list_)\n",
    "    query_read = f\"\"\"SELECT DISTINCT a.CUSTOMER_KEY, a.INVOICE_HEADER \n",
    "                FROM VF_CID_SALES_TRANSACTION_ICT a\n",
    "                LEFT JOIN (SELECT value as 'INVOICE_HEADER' FROM STRING_SPLIT( '{list_}' , ',')) b\n",
    "                    ON a.INVOICE_HEADER = b.INVOICE_HEADER\n",
    "                WHERE b.INVOICE_HEADER IS NOT NULL\"\"\"       \n",
    "    list_cus_inv = pd.read_sql(query_read, azr_engine)\n",
    "    list_cus_inv_out = pd.concat([list_cus_inv_out, list_cus_inv])\n",
    "list_cus_inv_out['INVOICE_HEADER']=list_cus_inv_out['INVOICE_HEADER'].apply(str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>CUSTOMER_KEY</th>\n",
       "      <th>INVOICE_HEADER</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1771569</td>\n",
       "      <td>45297037</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2395395</td>\n",
       "      <td>45290649</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2475783</td>\n",
       "      <td>45289227</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2584624</td>\n",
       "      <td>45296343</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2830357</td>\n",
       "      <td>45289449</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   CUSTOMER_KEY INVOICE_HEADER\n",
       "0       1771569       45297037\n",
       "1       2395395       45290649\n",
       "2       2475783       45289227\n",
       "3       2584624       45296343\n",
       "4       2830357       45289449"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list_cus_inv_out.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Has no duplicate, can use\n",
      "Len of df: 6,250\n",
      "Length of import = 4,054\n",
      "Number of records = 4,054\n",
      "1 | 4,054 | 4,054\n",
      "Start importing data customer\n",
      "Import Done\n"
     ]
    }
   ],
   "source": [
    "## Check danh sách khách hàng để tạo DIM \n",
    "\n",
    "check_unique_cus = df[df['Lý do']=='Invoice']\\\n",
    "    .merge(\n",
    "        list_cus_inv_out, \n",
    "        left_on= 'Mã giao dịch', \n",
    "        right_on = 'INVOICE_HEADER', \n",
    "        how = 'left'\n",
    "        )\n",
    "df_mapping_cus      = check_unique_cus[check_unique_cus['CUSTOMER_KEY'].notna()][['CUSTOMER_KEY','Mã thành viên']].drop_duplicates()\n",
    "check_unique_cus    = df_mapping_cus.groupby('Mã thành viên', as_index=False).agg(NumCus = ('CUSTOMER_KEY','nunique')).sort_values(by='NumCus',ascending=False)\n",
    "len_not_unique      = len(check_unique_cus[check_unique_cus['NumCus']>1])\n",
    "\n",
    "if len_not_unique != 0:\n",
    "    print(\"Error, has not unique code\", len_not_unique)\n",
    "else:\n",
    "    print(\"Has no duplicate, can use\")\n",
    "    print(f\"Len of df: {len(df_mapping_cus):,.0f}\")\n",
    "\n",
    "## Check với DIM đã có trên server\n",
    "## Chỉ import những mã khách hàng chưa có\n",
    "\n",
    "query_read_cus = \"\"\"SELECT [Mã thành viên] FROM D_ICT_AKACHAIN_LOYALTY_CUS_MAPPING \"\"\"\n",
    "list_cus_old = pd.read_sql(query_read_cus, getEngine(SaleMart))\n",
    "df_mapping_cus = df_mapping_cus[~df_mapping_cus['Mã thành viên'].isin(list_cus_old['Mã thành viên'])]\n",
    "print(f\"Length of import = {len(df_mapping_cus):,.0f}\")\n",
    "\n",
    "\n",
    "## Get basic Info of customer base on new customer\n",
    "if len(df_mapping_cus) > 0:\n",
    "    num_part = math.ceil(len(df_mapping_cus) /15_000)\n",
    "    list_cus_get_info = df_mapping_cus['CUSTOMER_KEY'].astype(str)\n",
    "    list_cus_info_out = pd.DataFrame()\n",
    "    print(f\"Number of records = {len(df_mapping_cus):,.0f}\")\n",
    "    for i in range(1, num_part+1):\n",
    "        list_ = list_cus_get_info[(i-1)*15_000 : min(i * 15_000, len(list_cus_get_info))]\n",
    "        print(f\"{i} | {min(i * 15_000, len(list_cus_get_info)):,.0f} | {len(list_):,.0f}\")\n",
    "        list_ = ','.join(list_)\n",
    "        query_read_info = f\"\"\"SELECT [CUSTOMER_KEY], [CUSTOMER_NAME], [CUSTOMER_ADDRESS], [CUSTOMER_PHONE] \n",
    "                        FROM [dbo].[VD_CID_CUSTOMER_ICT] WHERE CUSTOMER_KEY IN ( {list_} )\"\"\"\n",
    "        list_cus_info = pd.read_sql(query_read_info, azr_engine)\n",
    "        list_cus_info_out = pd.concat([list_cus_info_out, list_cus_info])\n",
    "\n",
    "if len(df_mapping_cus) > 0 : \n",
    "    print(\"Start importing data customer\")\n",
    "    df_mapping_cus\\\n",
    "        .merge(list_cus_info_out, on='CUSTOMER_KEY', how='left')\\\n",
    "        .to_sql(\"D_ICT_AKACHAIN_LOYALTY_CUS_MAPPING\", getEngine(SaleMart), if_exists='append', index=False)\n",
    "    print(\"Import Done\")\n",
    "else: \n",
    "    print(\"Not import because duplicate\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "query_read = f\"\"\"SELECT \n",
    "                DISTINCT [Ngày tạo], [Ngày giao dịch], [Mã giao dịch] \n",
    "                FROM [FLC_SHOP_SALES_DATAWAREHOUSE].[dbo].[F_ICT_AKACHAIN_LOYALTY] \n",
    "                WHERE [Ngày tạo] >=  '{str(min_date_filter_).split(' ',1)[0]}'\n",
    "                \"\"\"\n",
    "history = pd.read_sql(query_read, getEngine(SaleMart))\n",
    "history['Ngày tạo']         = pd.to_datetime(history['Ngày tạo'])\n",
    "history['Ngày giao dịch']   = pd.to_datetime(history['Ngày giao dịch'])\n",
    "history['Flg']              = 1\n",
    "if len(history) > 0:\n",
    "    history.sample(5)\n",
    "else:\n",
    "    print(\"Df null\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of duplicate: 0\n",
      "Number of not duplicate: 8,627\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Mã thành viên</th>\n",
       "      <th>Ngày tạo</th>\n",
       "      <th>Ngày giao dịch</th>\n",
       "      <th>Lý do</th>\n",
       "      <th>Mã giao dịch</th>\n",
       "      <th>Số tiền mua</th>\n",
       "      <th>Điểm xếp hạng quy đổi</th>\n",
       "      <th>Điểm phát sinh</th>\n",
       "      <th>Điểm đổi quà đã sử dụng</th>\n",
       "      <th>TIME_INSERT</th>\n",
       "      <th>Source</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1269</th>\n",
       "      <td>e99db969-d3de-4ba9-a1d5-e4e5f1e0879f</td>\n",
       "      <td>2023-06-29</td>\n",
       "      <td>2023-06-29</td>\n",
       "      <td>Invoice</td>\n",
       "      <td>45295716</td>\n",
       "      <td>287000.0</td>\n",
       "      <td>71</td>\n",
       "      <td>71</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2023-06-30 09:17:44.730910</td>\n",
       "      <td>Data_20230629_20230629</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8411</th>\n",
       "      <td>01e4c634-e960-4325-b1db-b0b2fb4b2f57</td>\n",
       "      <td>2023-06-29</td>\n",
       "      <td>2023-06-29</td>\n",
       "      <td>Invoice</td>\n",
       "      <td>45286255</td>\n",
       "      <td>135000.0</td>\n",
       "      <td>33</td>\n",
       "      <td>33</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2023-06-30 09:17:44.730910</td>\n",
       "      <td>Data_20230629_20230629</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7009</th>\n",
       "      <td>1600a3fd-7d27-4ce8-9c73-20439c9d5f03</td>\n",
       "      <td>2023-06-29</td>\n",
       "      <td>2023-06-29</td>\n",
       "      <td>Invoice</td>\n",
       "      <td>45288437</td>\n",
       "      <td>2390000.0</td>\n",
       "      <td>597</td>\n",
       "      <td>597</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2023-06-30 09:17:44.730910</td>\n",
       "      <td>Data_20230629_20230629</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                             Mã thành viên   Ngày tạo Ngày giao dịch    Lý do  \\\n",
       "1269  e99db969-d3de-4ba9-a1d5-e4e5f1e0879f 2023-06-29     2023-06-29  Invoice   \n",
       "8411  01e4c634-e960-4325-b1db-b0b2fb4b2f57 2023-06-29     2023-06-29  Invoice   \n",
       "7009  1600a3fd-7d27-4ce8-9c73-20439c9d5f03 2023-06-29     2023-06-29  Invoice   \n",
       "\n",
       "     Mã giao dịch  Số tiền mua  Điểm xếp hạng quy đổi  Điểm phát sinh  \\\n",
       "1269     45295716     287000.0                     71              71   \n",
       "8411     45286255     135000.0                     33              33   \n",
       "7009     45288437    2390000.0                    597             597   \n",
       "\n",
       "      Điểm đổi quà đã sử dụng                TIME_INSERT  \\\n",
       "1269                      NaN 2023-06-30 09:17:44.730910   \n",
       "8411                      NaN 2023-06-30 09:17:44.730910   \n",
       "7009                      NaN 2023-06-30 09:17:44.730910   \n",
       "\n",
       "                      Source  \n",
       "1269  Data_20230629_20230629  \n",
       "8411  Data_20230629_20230629  \n",
       "7009  Data_20230629_20230629  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "output = df.merge(\n",
    "    history, \n",
    "    on = ['Ngày tạo', 'Ngày giao dịch','Mã giao dịch'], \n",
    "    how = 'left')\n",
    "print(f\"Number of duplicate: {len(output[output['Flg']==1]):,.0f}\")\n",
    "print(f\"Number of not duplicate: {len(output[output['Flg']!=1]):,.0f}\")\n",
    "\n",
    "output = output[output['Flg']!=1]\n",
    "output = output.drop(columns='Flg')\n",
    "if len(output)>0:\n",
    "    display(output.sample(3))\n",
    "else:\n",
    "    display(\"df Null\")\n",
    "    display(output.sample(3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "-1"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output.to_sql(\n",
    "    \"F_ICT_AKACHAIN_LOYALTY\", \n",
    "    getEngine(SaleMart), \n",
    "    if_exists='append', \n",
    "    index=False\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total records to import 32,787\n",
      "Done import\n"
     ]
    }
   ],
   "source": [
    "## 15/03/2023 import thêm mapping return lên \n",
    "## Phải chạy code này mỗi lần import\n",
    "\n",
    "query_read = f\"\"\"\n",
    "            SELECT  \n",
    "                DISTINCT a.INVOICE_HEADER, a.MEMO_HEADER\n",
    "                , CAST(a.SALES_ORDER_DATE AS DATE) AS ReturnDate\n",
    "                , CAST(a.TRANSACTION_DATE AS DATE) AS ReturnTransDate\n",
    "                , CAST(b.SALES_ORDER_DATE AS DATE) AS SalesDate\n",
    "                , CAST(b.TRANSACTION_DATE AS DATE) AS SalesTransDate\n",
    "            FROM [VF_CID_SALES_TRANSACTION_ICT] a\n",
    "            LEFT JOIN [VF_CID_SALES_TRANSACTION_ICT]  b\n",
    "                ON a.INVOICE_HEADER = b.INVOICE_HEADER AND b.TRANSACTION_TYPE = 'Sales Invoice'\n",
    "            WHERE \n",
    "                a.DATE_KEY > 20220701\n",
    "                AND a.MEMO_HEADER IS NOT NULL \n",
    "                AND a.MEMO_HEADER <> 0\n",
    "            \"\"\"\n",
    "df_mapping = pd.read_sql(query_read, azr_engine)\n",
    "total_records_import = len(df_mapping)\n",
    "print(f\"Total records to import {total_records_import:,.0f}\")\n",
    "df_mapping.to_sql(\"F_ICT_AKACHAIN_MAPPING_RETURN\", getEngine(SaleMart), if_exists='replace', index=False)\n",
    "\n",
    "print(\"Done import\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total records to import 8,576\n",
      "Done import\n"
     ]
    }
   ],
   "source": [
    "# 04/05/2023 Import thêm mã GRPO để mapping\n",
    "query_read = f\"\"\"\n",
    "                SELECT \n",
    "                    GRPO_HEADER, CUSTOMER_CODE, SHOP_CODE, INVOICE_HEADER\n",
    "                FROM VF_CID_GRPO_TRANSACTION_ICT\n",
    "                WHERE SHOP_CODE IS NOT NULL\n",
    "                \"\"\"\n",
    "df_grpo = pd.read_sql(query_read, azr_engine)\n",
    "total_records_import = len(df_grpo)\n",
    "print(f\"Total records to import {total_records_import:,.0f}\")\n",
    "df_grpo.to_sql(\"F_ICT_AKACHAIN_MAPPING_GRPO\", getEngine(SaleMart), if_exists='replace', index=False)\n",
    "print(\"Done import\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hết phần import mỗi tuần\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Job audit dữ liệu định kỳ"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 1: Read dữ liệu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import polars as pl\n",
    "# Read \n",
    "base_dir = r'C:\\Users\\Admin\\OneDrive\\FRT\\6. Stored Data\\AkachainMember\\Project_AnualAudit'\n",
    "dir_number = r'01_20230609'\n",
    "\n",
    "all_path = []\n",
    "for root, dirs, files in os.walk(os.path.join(base_dir,dir_number)):\n",
    "    for file in files:\n",
    "        all_path.append(os.path.join(base_dir, dir_number, file))\n",
    "\n",
    "dfs = []\n",
    "for _p in range(len(all_path)):    \n",
    "        df = pl.read_excel(all_path[_p])\n",
    "        df = df.with_columns(pl.lit(_p).alias('file_num'))\n",
    "        dfs.append(df)\n",
    "        print(f'\\r Done get file number {_p}', end = '')\n",
    "\n",
    "merged_df = pl.concat(dfs, )\n",
    "merged_df = merged_df.with_columns(pl.col('Ngày tạo').str.strptime(pl.Date, fmt='%Y-%m-%d', strict=False).alias('Ngày tạo'))\n",
    "merged_df = merged_df.with_columns(pl.col('Ngày giao dịch').str.strptime(pl.Date, fmt='%Y-%m-%d', strict=False).alias('Ngày giao dịch'))\n",
    "merged_df = merged_df.with_column(pl.col(\"Ngày giao dịch\").dt.strftime(\"%m\").alias(\"month\"))\n",
    "merged_df.head() \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 2: Check export duplicates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check export có bị lỗi không\n",
    "grouped_df = merged_df.groupby([pl.col('file_num'),pl.col('month')]).count().sort('file_num')\n",
    "if len(grouped_df) == len(all_path):\n",
    "    print(f\"OK, not duplicated\")\n",
    "    print(grouped_df.head())\n",
    "else:\n",
    "    print(f\"Error Duplicated\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 3: Read history SQL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "min_date = '2023-01-01'\n",
    "max_date = '2023-05-31'\n",
    "\n",
    "query_read = f\"\"\"\n",
    "    SELECT  [Mã thành viên]\n",
    "      ,[Ngày tạo]\n",
    "      ,[Ngày giao dịch]\n",
    "      ,[Lý do]\n",
    "      ,[Mã giao dịch]\n",
    "      ,[Điểm phát sinh]\n",
    "        FROM [FLC_SHOP_SALES_DATAWAREHOUSE].[dbo].[F_ICT_AKACHAIN_LOYALTY]\n",
    "        WHERE [Ngày giao dịch] BETWEEN '{min_date}' AND '{max_date}'\n",
    "\"\"\"\n",
    "df_sql = pd.read_sql(query_read, getEngine(SaleMart))\n",
    "df_sql = df_sql.astype({'Ngày tạo':'datetime64[ns]', 'Ngày giao dịch':'datetime64[ns]'})\n",
    "df_sql.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# So sánh df read_sql có duplicate không\n",
    "length_df = len(df_sql)\n",
    "length_df_dropdup = len(df_sql.drop_duplicates())\n",
    "print(f\"Length df: {length_df:,.0f}\")\n",
    "print(f\"Length df drop duplicate: {length_df_dropdup:,.0f}\")\n",
    "print(f\"Different = {length_df_dropdup - length_df :,.0f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Chuyển df Polars sang df_pandas\n",
    "df_pd = merged_df.to_pandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "col_to_merge = ['Mã thành viên', 'Ngày tạo', 'Ngày giao dịch', 'Lý do', 'Mã giao dịch','Điểm phát sinh']\n",
    "\n",
    "df_ss_pd = df_pd[col_to_merge]\n",
    "df_ss = df_ss_pd.merge(df_sql, how = 'outer', indicator=True)\n",
    "\n",
    "# Loại trừ mã khách hàng đặc biệt\n",
    "df_special = df_ss[df_ss['Mã thành viên'] == '45605402-37aa-4fef-9cbb-6938256a372f']\n",
    "df_ss = df_ss[df_ss['Mã thành viên'] != '45605402-37aa-4fef-9cbb-6938256a372f']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Stats:\n",
    "df_ss.groupby('_merge').agg(_count =('_merge','count'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "col_to_merge_2 = ['Mã thành viên', 'Ngày tạo', 'Ngày giao dịch', 'Lý do', 'Mã giao dịch']\n",
    "\n",
    "df_ss_pd = df_pd[col_to_merge]\n",
    "df_ss = df_ss_pd.merge(df_sql, how = 'outer', on = col_to_merge_2, indicator=True, suffixes=['_pd', '_sql'])\n",
    "\n",
    "df_ss['diff'] = df_ss['Điểm phát sinh_pd'].fillna(0) - df_ss['Điểm phát sinh_sql'].fillna(0)\n",
    "df_ss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_ss[df_ss['diff']!=0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Result to import"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# DF cuối cùng để import \n",
    "\n",
    "col_to_merge = ['Mã thành viên', 'Ngày tạo', 'Ngày giao dịch', 'Lý do', 'Mã giao dịch','Điểm phát sinh']\n",
    "\n",
    "df_ss_pd = df_pd\n",
    "df_ss = df_ss_pd.merge(df_sql, how = 'outer', indicator=True)\n",
    "\n",
    "# Loại trừ mã khách hàng đặc biệt\n",
    "df_special = df_ss[df_ss['Mã thành viên'] == '45605402-37aa-4fef-9cbb-6938256a372f']\n",
    "df_ss = df_ss[df_ss['Mã thành viên'] != '45605402-37aa-4fef-9cbb-6938256a372f']\n",
    "df_ss = df_ss[df_ss['_merge']!='both']\n",
    "df_ss['TIME_INSERT'] = datetime.now()\n",
    "df_ss['Source'] = 'Audit import ' + dir_number\n",
    "df_ss.drop(columns=['_merge','file_num', 'month',],  inplace=True)\n",
    "\n",
    "df_ss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import final\n",
    "df_ss.to_sql(\n",
    "    \"F_ICT_AKACHAIN_LOYALTY\", \n",
    "    getEngine(SaleMart), \n",
    "    if_exists='append', \n",
    "    index=False\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_ss_pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(df_ss.columns)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Rewrite code import with identity column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ngày 04/03/2023:\n",
    "# Kỹ thuật identity column cho sql server để load dữ liệu nhanh hơn\n",
    "\n",
    "# Read lại bảng cũ\n",
    "df_import_cus = pd.read_sql(\"SELECT  * FROM [D_ICT_AKACHAIN_LOYALTY_CUS_MAPPING]\", getEngine(SaleMart))\n",
    "print('Done ')\n",
    "# Import vào bảng đó\n",
    "df_import_cus.to_sql('D_ICT_AKACHAIN_LOYALTY_CUS_MAPPING_v2', getEngine(SaleMart), index=False, if_exists='append')\n",
    "\n",
    "# Dùng insert nhanh hơn đó\n",
    "#\n",
    "\n",
    "\"DELETE FROM [D_ICT_AKACHAIN_LOYALTY_CUS_MAPPING_v2]\"\n",
    "\n",
    "\n",
    "\"DBCC CHECKIDENT ('D_ICT_AKACHAIN_LOYALTY_CUS_MAPPING_v2', RESEED, 0)\"\n",
    "\n",
    "# Note check lại cách sử dụng bảng dim customer đã ổn chưa vì có nhiều trường hợp null, nên chăng đánh key từ bảng fact chứ ko phải DIM\n",
    "# >> Không cần thiết phải đánh từ bảng DIM, mà là seeding sẵn vào bảng, nhưng khi có thông tin mới \n",
    "# >> \n",
    "\n",
    "# Flow : \n",
    "# 1."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Flow\n",
    "1. Load data từ files hàng tuần\n",
    "2. Check thông tin customer đó trên Azure (Transactions, key = BuyingInvoice_header)\n",
    "3. Import thông tin customer vào Dim Customer những customer chưa được flag là completed và đánh identity nếu không có thì flag là completed\n",
    "4. Import data AKC details vào DB\n",
    "\n",
    "Note: Cẩn thận code làm sao nếu broken 1 tuần thì không phải cần chạy lại all data\n",
    "\n",
    "Nhu cầu cuối cùng: Khi load bảng details lên Power BI thì chỉ cần load theo DIM là cột identity"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generate danh sách file để chạy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Danh sách file để chạy\n",
    "# Check lại trước khi chạy full log\n",
    "import os, glob\n",
    "import polars as pl\n",
    "import pandas as pd\n",
    "dir = r\"C:/Users/Admin/OneDrive/FRT/6. Stored Data/AkachainMember/Data/\"\n",
    "file_list = glob.glob(os.path.join(dir, \"*.xlsx\"))\n",
    "# file_list = [i for i in file_list if 'Data_' in i or 'Import' in i]\n",
    "list_f= []\n",
    "list_l= []\n",
    "for f in file_list:\n",
    "     if 'Data_' in f or 'Import' in f or 'Data.' in f:\n",
    "        df = pl.read_excel(f,  read_csv_options={\"infer_schema_length\": None})\n",
    "        list_f.append(f.split(\"\\\\\",-1)[1])\n",
    "        list_l.append(df.height)\n",
    "\n",
    "df_to_import = pd.DataFrame(zip(list_f, list_l), columns=('file', 'length'))\n",
    "exclude_list = ['Data_20220812_20220821.xlsx']\n",
    "df_to_import = df_to_import[~df_to_import['file'].isin(exclude_list)]\n",
    "\n",
    "# File Data_20220812_20220821 Đã nằm trong Data nên không cần import nó \n",
    "# Mà chỉ cần import file Data\n",
    "# File cuối cùng còn thiếu là missingDoc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_to_import[11:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_import_akc_v2(_file_to_import):\n",
    "\n",
    "    import polars as pl\n",
    "\n",
    "    # source_ = 'Data_20220822_20220828'\n",
    "    # source_ = 'Data_20230109_20230115'\n",
    "    source_ = _file_to_import\n",
    "    def proc_etl_pl(source_):\n",
    "        df_pl = pl.read_excel(r\"C:/Users/Admin/OneDrive/FRT/6. Stored Data/AkachainMember/Data/\" + source_ , read_csv_options={\"infer_schema_length\": 0})\n",
    "        df_pl = df_pl.with_columns(pl.col('Ngày tạo').str.strptime(pl.Date, fmt = '%Y-%m-%d').cast(pl.Date))\n",
    "        df_pl = df_pl.with_columns(pl.col('Ngày giao dịch').str.strptime(pl.Date, fmt = '%Y-%m-%d').cast(pl.Date))\n",
    "        df_pl = df_pl.with_columns(pl.col('Điểm phát sinh').cast(pl.Int64))\n",
    "        df_pl = df_pl.with_columns(pl.col('Điểm xếp hạng quy đổi').cast(pl.Int64))\n",
    "        df_pl = df_pl.with_columns(pl.col('Số tiền mua').cast(pl.Float64)).with_columns(pl.col('Số tiền mua').cast(pl.Int64))\n",
    "\n",
    "        df_pl = df_pl.rename({\n",
    "                    'Mã thành viên'             : 'customer_code_akc',\n",
    "                    'Ngày tạo'                  : 'created_date',\n",
    "                    'Ngày giao dịch'            : 'transaction_date',\n",
    "                    'Lý do'                     : 'transaction_type_name',\n",
    "                    'Mã giao dịch'              : 'transaction_code',\n",
    "                    'Số tiền mua'               : 'amount', \n",
    "                    'Điểm xếp hạng quy đổi'     : 'point_ranking',  \n",
    "                    'Điểm phát sinh'            : 'point',\n",
    "                    })\n",
    "        df_pl = df_pl.drop('Điểm đổi quà đã sử dụng')\n",
    "        df_pl = df_pl.with_columns(pl.lit(datetime.now()).alias('time_insert'))\n",
    "        df_pl = df_pl.with_columns(pl.lit(source_).alias('source'))\n",
    "        return df_pl\n",
    "\n",
    "    df_akc = proc_etl_pl(source_)\n",
    "\n",
    "    length_df_begin = df_akc.height\n",
    "    # Step 1: ETL from source\n",
    "    min_date_filter = df_akc['created_date'].min()\n",
    "    print(f'Min date filter is {min_date_filter}')\n",
    "\n",
    "    # Information of  server\n",
    "    USER_NAME   = 'ecom_user'\n",
    "    PW          = 'Ec0m@12345'\n",
    "    SERVER      = '118.69.201.34'\n",
    "    DATABASE    = 'FLC_SHOP_SALES_DATAWAREHOUSE'\n",
    "    conn_dw     = f\"mssql://{USER_NAME}:{PW}@{SERVER}/{DATABASE}?driver=SQL+Server\"\n",
    "\n",
    "\n",
    "    display(df_akc.head(1))\n",
    "\n",
    "\n",
    "    print('|| Step 2:')\n",
    "    # Step 2: \n",
    "    # Những hiểu biết trước thì 1 mã AKC chỉ có 1 Mã Customer Key nên không cần check lại\n",
    "    # Nhưng thực tế bảng Loyalty của AKC thì có trường hợp 1 akc_cus_code = 2 customer_key\n",
    "\n",
    "    # Lấy ra danh sách Mã thành viên AKC chưa có trong DIM\n",
    "    query_dim_cus = \"\"\"SELECT DISTINCT key_id,customer_code_akc, completed_flg FROM D_ICT_AKACHAIN_LOYALTY_CUS_MAPPING_v2\"\"\"\n",
    "    dim_cus = pl.read_sql(query_dim_cus, conn_dw)\n",
    "\n",
    "    # Danh sách mã thành viên chưa có DIM Cus\n",
    "    df_get_cus_info = df_akc.join(dim_cus, on = 'customer_code_akc', how='left', )\n",
    "\n",
    "    # Print stats:\n",
    "    display(df_get_cus_info\\\n",
    "        .groupby('completed_flg')\\\n",
    "        .agg(pl.n_unique('customer_code_akc').alias('count_code')))\n",
    "\n",
    "    # Filter chỉ lấy những customer nào chưa completed\n",
    "    list_inv = df_get_cus_info\\\n",
    "                    .filter( (pl.col('transaction_type_name')=='Invoice') & (pl.col('completed_flg')!=1) )\\\n",
    "                    .select(pl.col('transaction_code')).unique() # Danh sách inv sẽ search\n",
    "    list_inv = list_inv['transaction_code'].to_list()\n",
    "    print(f\"Total List to get: {len(list_inv):,.0f}\")\n",
    "    list_cus_inv_out = pd.DataFrame()\n",
    "    num_part = math.ceil(len(list_inv) /15_000) # Chia tối đa read 15,000 records/lần\n",
    "\n",
    "    print(\"Get list Customer with Invoice\")\n",
    "    for i in range(1,num_part+1):\n",
    "        list_ = list_inv[(i-1)*15_000 : min(i * 15_000, len(list_inv))]\n",
    "        num_record = min(i * 15_000, len(list_inv))\n",
    "        print(f\"|_{i} | {num_record:,.0f} | {num_record / len(list_inv):,.1%}\")\n",
    "        list_ = ','.join(list_)\n",
    "\n",
    "        query_read = f\"\"\"SELECT DISTINCT a.CUSTOMER_KEY, a.INVOICE_HEADER \n",
    "                FROM VF_CID_SALES_TRANSACTION_ICT a\n",
    "                LEFT JOIN (SELECT value as 'INVOICE_HEADER' FROM STRING_SPLIT( '{list_}' , ',')) b\n",
    "                    ON a.INVOICE_HEADER = b.INVOICE_HEADER\n",
    "                WHERE b.INVOICE_HEADER IS NOT NULL\"\"\"\n",
    "\n",
    "        # Tạm thời read bẳng pandas vì connectorx chưa tìm ra cách connect tới azure sql server\n",
    "        list_cus_inv = pd.read_sql(query_read, azr_engine)\n",
    "        list_cus_inv_out = pd.concat([list_cus_inv_out, list_cus_inv])\n",
    "\n",
    "    # Do test trên SQL thì nếu left join luôn DIM customer thì sẽ chậm hơn việc SELECT IN list customer_key\n",
    "    # Nên chia làm 2 step : 1 là lấy ra danh sách cus, 2 là lấy ra thông tin cus\n",
    "\n",
    "    print('|| Step 3: Get customer information')\n",
    "\n",
    "    # Step 3: Get customer Information \n",
    "    n_cus_scan = list_cus_inv_out.CUSTOMER_KEY.nunique()\n",
    "\n",
    "    list_scan_cus = list_cus_inv_out.CUSTOMER_KEY.unique().tolist()\n",
    "    list_cus_info_out = pd.DataFrame()\n",
    "    num_part_cus = math.ceil(len(list_scan_cus) /15_000) # Chia tối đa read 15,000 records/lần\n",
    "\n",
    "    print(\"Step 3: Get list Customer information with CUSTOMER_KEY\")\n",
    "    print(\"Total customer to scan: \", f\"{(n_cus_scan):,.0f}\")\n",
    "\n",
    "    for i in range(1, num_part_cus+1):\n",
    "        list_ = list_scan_cus[(i-1)*15_000 : min(i * 15_000, len(list_scan_cus))]\n",
    "        num_record = min(i * 15_000, len(list_scan_cus))\n",
    "        print(f\"|_{i} | {num_record:,.0f} | {num_record / len(list_scan_cus):,.1%}\")\n",
    "        list_ = ','.join(str(v) for v in list_)\n",
    "        query_read = f\"\"\"SELECT CUSTOMER_KEY, CUSTOMER_NAME, CUSTOMER_ADDRESS, CUSTOMER_PHONE\n",
    "                        FROM VD_CID_CUSTOMER_ICT\n",
    "                        WHERE CUSTOMER_KEY IN \n",
    "                            (SELECT value as 'CUSTOMER_KEY' FROM STRING_SPLIT( '{list_}' , ',')) \n",
    "                    \"\"\"   \n",
    "        # Tạm thời read bẳng pandas vì connectorx chưa tìm ra cách connect tới azure sql server\n",
    "        list_cus_info = pd.read_sql(query_read, azr_engine)\n",
    "        list_cus_info_out = pd.concat([list_cus_info_out, list_cus_info])\n",
    "\n",
    "    # Check diff\n",
    "    print(f\"Total cus scan: {list_cus_inv_out.CUSTOMER_KEY.nunique():,.0f}\")\n",
    "    print(f\"Total cus result: {list_cus_info_out.CUSTOMER_KEY.nunique():,.0f}\")\n",
    "    print(f\"Diff : {list_cus_info_out.CUSTOMER_KEY.nunique() - list_cus_inv_out.CUSTOMER_KEY.nunique() :,.0f}\")\n",
    "\n",
    "\n",
    "\n",
    "    print('|| Step 4:')\n",
    "\n",
    "    # Step 4: Clean lại file và import to Dim Customer\n",
    "    df_import_cus = list_cus_inv_out.merge(list_cus_info_out, on ='CUSTOMER_KEY', how = 'left', indicator=True)\n",
    "    df_import_cus = pl.from_pandas(df_import_cus)\n",
    "    df_import_cus = df_import_cus.with_columns(pl.when(pl.col('_merge') == 'both').then(1).otherwise(0).alias('completed_flg'))\n",
    "    df_import_cus = df_import_cus.select(\n",
    "                                pl.col('CUSTOMER_KEY').alias('customer_key'), \n",
    "                                pl.col('INVOICE_HEADER').cast(pl.Utf8).alias('transaction_code'), \n",
    "                                pl.col('CUSTOMER_NAME').alias('customer_name'), \n",
    "                                pl.col('CUSTOMER_ADDRESS').alias('customer_address'),  \n",
    "                                pl.col('CUSTOMER_PHONE').alias('customer_phone'), \n",
    "                                pl.col('completed_flg'), )\n",
    "\n",
    "    df_import_cus = df_import_cus.join(df_get_cus_info\\\n",
    "        .filter((pl.col('transaction_type_name')=='Invoice') & (pl.col('completed_flg')!=1))\\\n",
    "        .select(pl.col('customer_code_akc'), pl.col('transaction_code'))\n",
    "        , how='outer'\n",
    "        , on='transaction_code'\n",
    "    )\n",
    "\n",
    "    # Có những document không tìm được trong db prod nên loại bỏ\n",
    "    # display(df_import_cus.filter(pl.col('customer_code_akc') == '04c21fe9-e07c-4730-9d92-38c6621b067d'))\n",
    "    # display(df_import_cus.filter(pl.col('customer_key').is_null()))\n",
    "    df_import_cus = df_import_cus.filter(~pl.col('customer_key').is_null())\n",
    "\n",
    "\n",
    "    df_import_cus = df_import_cus.drop('transaction_code').unique()\n",
    "    df_import_cus = df_import_cus.join(\n",
    "                    df_get_cus_info.select('customer_code_akc').unique()\n",
    "                    , how='outer'\n",
    "                    , on='customer_code_akc'\n",
    "    )\n",
    "    df_import_cus = df_import_cus.with_columns(pl.col('completed_flg').fill_null(0))\n",
    "    stats_null = df_import_cus.groupby('completed_flg').agg(pl.n_unique('customer_code_akc').alias('total_cus'))\n",
    "    display(stats_null)\n",
    "    print(\n",
    "    f\"\"\"Total cus import: {df_import_cus.select('customer_code_akc').n_unique():,.0f}\n",
    "    Total cus Scan: {stats_null['total_cus'].sum():,.0f}\n",
    "    ## Diff: {df_import_cus.select('customer_code_akc').n_unique() - stats_null['total_cus'].sum():,.0f}\n",
    "    \"\"\")\n",
    "    # Reorder columns\n",
    "    df_import_cus = df_import_cus[['customer_key', 'customer_code_akc', 'customer_name', 'customer_address', 'customer_phone', 'completed_flg']]\n",
    "    df_import_cus = df_import_cus.with_columns(pl.lit(datetime.now()).alias('time_insert'))\n",
    "    df_import_cus = df_import_cus.with_columns(pl.lit(source_).alias('source'))\n",
    "\n",
    "    # Trước khi import thì loại bỏ những customer đã có status = 1 trong db\n",
    "    len_before = len(df_import_cus)\n",
    "    print(f\"Before remove completed: {len_before:,.0f}\")\n",
    "    df_import_cus = df_import_cus.filter(\n",
    "                        ~pl.col('customer_code_akc').is_in(dim_cus.filter(pl.col('completed_flg') == True)['customer_code_akc']\n",
    "                        ))\n",
    "\n",
    "    dim_cus.filter(pl.col('completed_flg') == True).select('customer_code_akc').unique()\n",
    "    print(f\"After remove completed: {len(df_import_cus):,.0f} ({len(df_import_cus)-len_before:,.0f})\")\n",
    "\n",
    "    # Phải remove thêm 2 lần nữa\n",
    "\n",
    "    # Remove lần 1\n",
    "    # Nếu before là 0 , after là 0 thì không cần làm gì\n",
    "\n",
    "    df_ba = df_import_cus.select('customer_code_akc', 'completed_flg').join(dim_cus, on = 'customer_code_akc', how='left')\n",
    "\n",
    "    df_import_cus = df_import_cus.filter(\n",
    "                        ~pl.col('customer_code_akc').is_in(\n",
    "                            df_ba.filter( \n",
    "                                        (pl.col('completed_flg') == 0) & \n",
    "                                        (pl.col('completed_flg_right') ==  0)\n",
    "                                        )['customer_code_akc']\n",
    "                        ))\n",
    "\n",
    "    # Remove lần 2\n",
    "    # Nếu before là 0 , after là 1 thì update,\n",
    "    df_update = df_import_cus.join(dim_cus, on = 'customer_code_akc', how='left')\n",
    "    df_update = df_update.filter((pl.col('completed_flg') == 1) & (pl.col('completed_flg_right') == 0)).to_pandas()\n",
    "    if len(df_update) > 0:\n",
    "        values_list = []\n",
    "        for index, row in df_update.iterrows():\n",
    "            values_list.append(\n",
    "                (row['customer_key']\n",
    "                , row['customer_code_akc']\n",
    "                , row['customer_name']\n",
    "                , row['customer_address']\n",
    "                , row['customer_phone']\n",
    "                , row['completed_flg']\n",
    "                , row['time_insert']\n",
    "                , row['source']\n",
    "                , row['key_id'])\n",
    "                )\n",
    "        import pyodbc\n",
    "\n",
    "        # connect to SQL Server database\n",
    "        cnxn = pyodbc.connect(SaleMart)\n",
    "        cursor = cnxn.cursor()\n",
    "\n",
    "        # define update query\n",
    "        update_query = \"\"\"\n",
    "                    UPDATE D_ICT_AKACHAIN_LOYALTY_CUS_MAPPING_v2\n",
    "                    SET \n",
    "                        customer_key = ?, \n",
    "                        customer_code_akc = ?,\n",
    "                        customer_name = ?,\n",
    "                        customer_address = ?,\n",
    "                        customer_phone = ?,\n",
    "                        completed_flg = ?,\n",
    "                        time_insert = ?,\n",
    "                        source = ?\n",
    "                    WHERE key_id = ?\n",
    "                    \"\"\"\n",
    "\n",
    "        # execute update query for each set of values\n",
    "        for values in values_list:\n",
    "            cursor.execute(update_query, values)\n",
    "\n",
    "        # commit changes to database\n",
    "        cnxn.commit()\n",
    "\n",
    "        # close cursor and connection\n",
    "        cursor.close()\n",
    "        cnxn.close()\n",
    "\n",
    "        df_import_cus = df_import_cus.filter(\n",
    "                            ~pl.col('customer_code_akc').is_in(\n",
    "                                df_update['customer_code_akc'].to_list()\n",
    "                            ))\n",
    "\n",
    "    df_import_cus.to_pandas().to_sql(\"D_ICT_AKACHAIN_LOYALTY_CUS_MAPPING_v2\", getEngine(SaleMart), if_exists='append', index=False)\n",
    "\n",
    "    # Viết lại tiếp 1 step nữa là delete những records nào mà complete thay đổi khi chạy lại\n",
    "\n",
    "\n",
    "    print('|| Step 5:')\n",
    "    # Step 5: Import df vào bảng, tuy nhiên cần check xem có trùng ko\n",
    "    # Read data after datefilter\n",
    "    min_date_filter_ = df_akc['created_date'].min() - timedelta(days=5)\n",
    "    print(f\"Check data date after: {min_date_filter}\")\n",
    "\n",
    "    query_r_history = f\"\"\"SELECT \n",
    "                    DISTINCT created_date, transaction_date, transaction_code\n",
    "                    FROM F_ICT_AKACHAIN_LOYALTY_v2 \n",
    "                    WHERE created_date >=  '{str(min_date_filter_)}'\n",
    "                    \"\"\"\n",
    "    history = pl.read_sql(query_r_history, conn_dw)\n",
    "    history = history.with_columns(pl.lit(1).alias('flg'))\n",
    "    if len(history)>0:\n",
    "        display(history.sample(5))\n",
    "    else:\n",
    "        print(\"Df history is null\")\n",
    "\n",
    "    output = df_akc.join(history, on=['created_date', 'transaction_date', 'transaction_code'], how='left')\n",
    "\n",
    "    print(f\"Number of duplicate: {len(output.filter(pl.col('flg')==1)):,.0f}\")\n",
    "    print(f\"Number of not duplicate: {len(output.filter(pl.col('flg')!=1)):,.0f}\")\n",
    "\n",
    "    # Filter final\n",
    "    output = output.filter(pl.col('flg')!=1).drop('flg')\n",
    "    output.to_pandas().to_sql(\"F_ICT_AKACHAIN_LOYALTY_v2\", getEngine(SaleMart), if_exists='append', index=False)\n",
    "\n",
    "    print(f\"Done import \")\n",
    "    print(f\"Length df begin: {length_df_begin:,.0f}\")\n",
    "    print(f\"Length df final: {output.height:,.0f}\")\n",
    "    print(f\"Diff length df {length_df_begin - output.height:,.0f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for f in df_to_import['file'][21:47]:\n",
    "    print('===========================================')\n",
    "    print(f'Run file is {f}')\n",
    "    run_import_akc_v2(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_to_import"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Câu để check 2 table có diff không\n",
    "\n",
    "\"\"\"\n",
    "  SELECT DISTINCT a.customer_code_akc , b.customer_code_akc\n",
    "  FROM F_ICT_AKACHAIN_LOYALTY_v2 a\n",
    "  LEFT join D_ICT_AKACHAIN_LOYALTY_CUS_MAPPING_v2  b\n",
    "  ON a.customer_code_akc = b.customer_code_akc\n",
    "  WHERE b.customer_code_akc is null\n",
    "\n",
    "\"\"\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_to_import[1:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check lại trước khi chạy full log\n",
    "import os, glob\n",
    "import polars as pl\n",
    "import pandas as pd\n",
    "dir = r\"C:/Users/Admin/OneDrive/FRT/6. Stored Data/AkachainMember/Data/\"\n",
    "file_list = glob.glob(os.path.join(dir, \"*.xlsx\"))\n",
    "# file_list = [i for i in file_list if 'Data_' in i or 'Import' in i]\n",
    "list_f= []\n",
    "list_l= []\n",
    "for f in file_list:\n",
    "     if 'Data_' in f or 'Import' in f or 'Data.' in f:\n",
    "        df = pl.read_excel(f,  read_csv_options={\"infer_schema_length\": None})\n",
    "        list_f.append(f.split(\"\\\\\",-1)[1])\n",
    "        list_l.append(df.height)\n",
    "\n",
    "df_to_import = pd.DataFrame(zip(list_f, list_l), columns=('file', 'length'))\n",
    "exclude_list = ['Data_20220812_20220821.xlsx']\n",
    "df_to_import = df_to_import[~df_to_import['file'].isin(exclude_list)]\n",
    "\n",
    "# File Data_20220812_20220821 Đã nằm trong Data nên không cần import nó \n",
    "# Mà chỉ cần import file Data\n",
    "# File cuối cùng còn thiếu là missingDoc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DONE\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## RECHECK DATA\n",
    "\n",
    "source_ = 'AKC_Revenue_Detailed_30.08-13.09_ICT'\n",
    "df = pd.read_csv(r\"C:/Users/Admin/OneDrive/FRT/6. Stored Data/AkachainMember/Weeklycheck/\" + source_ + \".csv\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "list_inv = df['OriginalTransCodeOfBu'].drop_duplicates()\n",
    "num_part = math.ceil(len(list_inv) /15000)\n",
    "\n",
    "df_out = pd.DataFrame()\n",
    "print(f\"Number of records = {len(list_inv)}\")\n",
    "for i in range(1,num_part+1):\n",
    "    list_ = list_inv[(i-1)*15000 : min(i * 15000, len(list_inv))]\n",
    "    print(i, min(i * 15000, len(list_inv)), len(list_))\n",
    "    list_ = ','.join(list_.astype(str))\n",
    "    query_read = f\"\"\"SELECT *\n",
    "                    FROM [dbo].[VF_CID_SALES_TRANSACTION_ICT]\n",
    "                    WHERE 1=1\n",
    "                        AND [W_DELETE_FLG] = 'N'\n",
    "                        AND IS_PROJECT = 'N'\n",
    "                        AND ORDER_TYPE NOT IN (N'Xuất trả NCC', N'Bán nội bộ', 'SO Service')\n",
    "                        AND CUSTOMER_KEY NOT IN (1480086, 24235209, 5327471, 16957226) -- Long Châu, FPT\n",
    "                        AND SHOP_KEY NOT IN (1667,648,10720,903,1224,1466,1552)\n",
    "                        AND INVOICE_HEADER IN ( {list_} )\"\"\"\n",
    "    df_read = pd.read_sql(query_read, azr_engine)\n",
    "    df_out = pd.concat([df_out, df_read])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_out[df_out['INVOICE_HEADER'] == ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_out.to_csv(r\"C:/Users/Admin/OneDrive/FRT/6. Stored Data/AkachainMember/Weeklycheck/OutCheck.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Import Danh sách khách hàng Inactive\n",
    "\n",
    "source_ = 'List_Inactive_Member_221004'\n",
    "df_inactive = pd.read_excel(r\"C:/Users/Admin/OneDrive/FRT/6. Stored Data/AkachainMember/Data/\" + source_ + \".xlsx\")\n",
    "df_inactive.to_sql(\"D_ICT_AKACHAIN_LOYALTY_CUS_INACTIVE\", getEngine(SaleMart), if_exists='append', index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Check lại data 34 và Azure\n",
    "f_date = '2022-10-01'\n",
    "t_date = '2022-10-10'\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Danh sách inactive\n",
    "query_inactive = f\"\"\"SELECT   [Mã thành viên], CUSTOMER_KEY\n",
    "  FROM D_ICT_AKACHAIN_LOYALTY_CUS_MAPPING\n",
    "  WHERE [Mã thành viên] IN (SELECT [Mã thành viên] FROM D_ICT_AKACHAIN_LOYALTY_CUS_INACTIVE)\n",
    "                \"\"\"\n",
    "df_inactive = pd.read_sql(query_inactive, getEngine(SaleMart))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "query_read_34 = f\"\"\"SELECT  a.[Mã thành viên]\n",
    "                    , b.CUSTOMER_KEY\n",
    "                    ,[Ngày tạo]\n",
    "                    ,[Ngày giao dịch]\n",
    "                    ,[Lý do]\n",
    "                    ,[Mã giao dịch]\n",
    "                    ,[Số tiền mua]\n",
    "                    ,[Điểm xếp hạng quy đổi]\n",
    "                    ,[Điểm phát sinh]\n",
    "                FROM [FLC_SHOP_SALES_DATAWAREHOUSE].[dbo].[F_ICT_AKACHAIN_LOYALTY] a\n",
    "                LEFT JOIN D_ICT_AKACHAIN_LOYALTY_CUS_MAPPING b\n",
    "                    ON a.[Mã thành viên] = b.[Mã thành viên]\n",
    "                WHERE [Lý do] = 'Invoice'\n",
    "                AND [Ngày giao dịch] BETWEEN '{f_date}' AND '{t_date}'\n",
    "                \"\"\"\n",
    "df_aka = pd.read_sql(query_read_34, getEngine(SaleMart))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Đọc data từ Azure\n",
    "query_read_az = f\"\"\"SELECT INVOICE_HEADER, CUSTOMER_KEY, SUM(LIST_AMT_AF_VAT) AS AMOUNT\n",
    "    FROM VF_CID_SALES_TRANSACTION_ICT\n",
    "    WHERE TRANSACTION_TYPE = 'Sales Invoice'\n",
    "        AND  [W_DELETE_FLG] = 'N'\n",
    "        AND CUSTOMER_KEY NOT IN (1480086, 24235209, 5327471, 16957226) -- Long Châu, FPT\n",
    "\n",
    "        AND CUSTOMER_KEY NOT IN (15993818) -- Đơn hàng truy thu\n",
    "--        AND PRODUCT_KEY NOT IN (SELECT PRODUCT_KEY      FROM VD_CID_PRODUCT WHERE W_DATASOURCE_ID = 'ICT' AND [PRODUCT_CATEGORY_NAME]   IN  ('DV', N'SUBSIDY')) -- Thẻ cào\n",
    "        AND  CONVERT(DATE, CONVERT(VARCHAR(10), [DATE_KEY])) BETWEEN '{f_date}' AND '{t_date}' \n",
    "        GROUP BY INVOICE_HEADER, CUSTOMER_KEY  \"\"\"\n",
    "df_az = pd.read_sql(query_read_az, azr_engine)\n",
    "df_az['AMOUNT'] = df_az['AMOUNT'].astype(int)\n",
    "\n",
    "# Loại bỏ những khách inactive\n",
    "df_az = df_az[~df_az['CUSTOMER_KEY'].isin(df_inactive['CUSTOMER_KEY'].values)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check duplicate giao dịch\n",
    "c_dup = df_aka.groupby('Mã giao dịch').agg(countL =('Mã giao dịch','count'))\n",
    "c_dup = c_dup[c_dup['countL']>1] \n",
    "n_dup = len(c_dup)\n",
    "print(f\"Số lượng line dup = {n_dup}\")\n",
    "display(c_dup)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "display(df_aka.sample(3), df_az.sample(3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_aka_agg = df_aka.groupby(['Mã giao dịch', 'CUSTOMER_KEY'], as_index=False).agg(amt_aka = ('Số tiền mua', 'sum'))\n",
    "df_aka_agg['Mã giao dịch'] = df_aka_agg['Mã giao dịch'].astype(int)\n",
    "df_aka_agg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_merge = df_az.merge(df_aka_agg, how='outer', left_on=['INVOICE_HEADER'], right_on=['Mã giao dịch'], suffixes=['_Az', '_AKA'])\n",
    "df_merge['AMOUNT'] = df_merge['AMOUNT'].fillna(0)\n",
    "df_merge['amt_aka'] = df_merge['amt_aka'].fillna(0)\n",
    "\n",
    "df_merge['DiffAmt'] = df_merge['AMOUNT'] - df_merge['amt_aka']\n",
    "df_diff = df_merge[(df_merge['CUSTOMER_KEY_Az']!=df_merge['CUSTOMER_KEY_AKA']) | (df_merge['DiffAmt']>1000)| (df_merge['DiffAmt']<-1000)]\n",
    "\n",
    "_so_line_diff =len(df_diff)\n",
    "display(len(df_diff))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Invoice_Header is na\n",
    "inv_na = df_diff[(df_diff['INVOICE_HEADER'].isna())]\n",
    "print(f\"Số INVOICE_HEADER is NA = {len(inv_na)} | AMT = {_convert_decimal(df_diff['amt_aka'].sum(),0)}\")\n",
    "display(inv_na.sample(3))\n",
    "gd_na = df_diff[(df_diff['Mã giao dịch'].isna())]\n",
    "print(f\"Số Mã gd is NA = {len(gd_na)} | AMT = {_convert_decimal(df_diff['AMOUNT'].sum(),0)}\")\n",
    "display(gd_na.sample(3))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "diff_amt = df_diff[~((df_diff['INVOICE_HEADER'].isna()) | (df_diff['Mã giao dịch'].isna()))]\n",
    "print(f\"Số Mã gd is NA = {len(diff_amt)} | AMT = {_convert_decimal(diff_amt['DiffAmt'].sum(),0)}\")\n",
    "display(diff_amt.sample(3))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Xem AMT = 0 thi thuoc cus nao\n",
    "df_diff[(df_diff['amt_aka'].isna()) | (df_diff['amt_aka']==0)].groupby('CUSTOMER_KEY_Az').agg(countInv =('INVOICE_HEADER','count')).sort_values(by='countInv', ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_diff.to_csv(r\"C:/Users/Admin/OneDrive/FRT/6. Stored Data/AkachainMember/Weeklycheck/OutCheck.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Summary Cho a Chương\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import glob, os \n",
    "import pandas_profiling as pp\n",
    "from HelperMaster import draw_hist\n",
    "dir = r\"C:\\Users\\Admin\\OneDrive\\FRT\\6. Stored Data\\AkachainMember\\Summary\\Data\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "list_file_tieu = []\n",
    "for file in os.listdir(dir):\n",
    "    if file.endswith(\".csv\"):\n",
    "        if 'tiêu' in file and 'ngày' in file:\n",
    "            file_dir = os.path.join(dir, file)\n",
    "            print(file)\n",
    "            list_file_tieu.append(file_dir)\n",
    "tieudiem = pd.concat([pd.read_csv(i) for i in list_file_tieu])\n",
    "\n",
    "list_file_tich = []\n",
    "for file in os.listdir(dir):\n",
    "    if file.endswith(\".csv\"):\n",
    "        if 'tích' in file and 'ngày' in file:\n",
    "            print(file)\n",
    "            file_dir = os.path.join(dir, file)\n",
    "            list_file_tich.append(file_dir)\n",
    "            # display(pd.read_csv(file_dir))\n",
    "tichdiem = pd.concat([pd.read_csv(i) for i in list_file_tich])\n",
    "\n",
    "tieudiem['Month'] = pd.to_datetime(tieudiem['DATE']).dt.month"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(tieudiem), len(tichdiem)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pp.ProfileReport(tieudiem)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tieudiem.groupby(['Month','GRANTED_MERCHANT_NAME', 'USED_MERCHANT_NAME']).agg(\n",
    "    countMemberID = ('MEMBER_ID', 'nunique'),\n",
    "    countMemberCode = ('MEMBER_CODE', 'nunique'),\n",
    "    totalPoint = ('TOTAL_FGOLD_AMOUNT_IN_DAY', 'sum'),\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tieudiem"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Def check\n",
    "dir = r\"C:\\Users\\Admin\\OneDrive\\FRT\\6. Stored Data\\AkachainMember\\Summary\\Data\"\n",
    "list_M = ['9', '10']\n",
    "\n",
    "df_tieu_details = pd.concat([pd.read_csv(dir + f\"/T{i} - Báo cáo tiêu điểm chi tiết trên từng giao dịch.csv\") for i in list_M])\n",
    "df_tieu_summary = pd.concat([pd.read_csv(dir + f\"/T{i} - Báo cáo tiêu điểm trên từng thành viên theo ngày.csv\") for i in list_M])\n",
    "df_tieu_details['DATE'] = pd.to_datetime(df_tieu_details['USED_TIME_GMT'], format=\"%Y-%m-%d\").dt.floor('d')\n",
    "df_tieu_summary['DATE'] = pd.to_datetime(df_tieu_summary['DATE'])\n",
    "\n",
    "df_tich_details = pd.concat([pd.read_csv(dir + f\"/T{i} - Báo cáo tích điểm chi tiết trên từng giao dịch.csv\") for i in list_M])\n",
    "df_tich_summary = pd.concat([pd.read_csv(dir + f\"/T{i} - Báo cáo tích điểm trên từng thành viên theo ngày.csv\") for i in list_M])\n",
    "df_tich_details = df_tich_details.drop(columns=['GRANT_TYPE', 'REASON', 'USAGE_PRIORITY','PARTNER_POINT_AMOUNT','POINT_EXCHANGE_RATE','CURRENCY_EXCHANGE_RATE',])\n",
    "df_tich_summary.drop(columns=['NAME', 'PHONE'],inplace=True)\n",
    "df_tich_summary.DATE = pd.to_datetime(df_tich_summary.DATE)\n",
    "\n",
    "df_tich_details[['VALID_DATE_GMT', 'EXPIRY_DATE_GMT', 'BUSINESS_TIME_GMT', 'CREATED_TIME_GMT']] = df_tich_details[['VALID_DATE_GMT', 'EXPIRY_DATE_GMT', 'BUSINESS_TIME_GMT', 'CREATED_TIME_GMT']].apply(pd.to_datetime)\n",
    " \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Valid_date và business date bằng nhau\n",
    "df_tich_details['Diff_time_create_business'] =  np.floor((df_tich_details.CREATED_TIME_GMT - df_tich_details.BUSINESS_TIME_GMT).dt.total_seconds()/3600)\n",
    "df_tich_details['Diff_time_valid_business'] =  np.floor((df_tich_details.VALID_DATE_GMT - df_tich_details.BUSINESS_TIME_GMT).dt.total_seconds())\n",
    "df_tich_details.sort_values(by='Diff_time_create_business', ascending=False)[:10]\n",
    "# df_tich_details['Diff_time'].hist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_tich_summary[df_tich_summary.USER_ADDRESS== 'f1306a1fc48778c4f8046eaf810bed8fd8e8f3']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pp.ProfileReport(df_tich_details)\n",
    "# df_tich_summary\n",
    "df_tich_details"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_dt_group = df_tieu_details.groupby(\n",
    "    ['MEMBER_CODE', 'USED_MERCHANT_NAME', 'GRANTED_MERCHANT_NAME', 'DATE'], as_index=False)\\\n",
    "    .agg(sumToken = ('TOKEN_AMOUNT', 'sum'), frequency = ('ORDER_CODE', 'nunique'))\n",
    "df_dt_group.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_tieu_summary.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_tieu_out = df_tieu_summary[['DATE', 'MEMBER_CODE','USED_MERCHANT_NAME','GRANTED_MERCHANT_NAME', 'TOTAL_TRANSACTION_IN_DAY', 'TOTAL_FGOLD_AMOUNT_IN_DAY']].merge(\n",
    "    df_dt_group,how='outer',\n",
    "    left_on=['DATE','MEMBER_CODE','USED_MERCHANT_NAME', 'GRANTED_MERCHANT_NAME'],\n",
    "    right_on=['DATE','MEMBER_CODE','USED_MERCHANT_NAME', 'GRANTED_MERCHANT_NAME'])\n",
    "df_tieu_out['Diff_Token'] = df_tieu_out.TOTAL_FGOLD_AMOUNT_IN_DAY.fillna(0) - df_tieu_out.sumToken.fillna(0)\n",
    "df_tieu_out['Diff_Trans'] = df_tieu_out.TOTAL_TRANSACTION_IN_DAY.fillna(0) - df_tieu_out.frequency.fillna(0)\n",
    "\n",
    "df_tieu_out.to_excel(dir + \"/Output.xlsx\", index=False)\n",
    "df_tieu_out.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df_summary.head(3)c\n",
    "# df_dt_group.head(3)8\n",
    "df_out.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_tieu_summary[df_tieu_summary.MEMBER_CODE=='db88b1ac-8a7f-4996-b525-5528afb71c6b']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_tieu_summary.groupby('GRANTED_MERCHANT_NAME')['TOTAL_FGOLD_AMOUNT_IN_DAY'].sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check điểm ngày 16/1 export\n",
    "import glob\n",
    "import pandas as pd \n",
    "dir = r\"C:\\Users\\Admin\\OneDrive\\FRT\\6. Stored Data\\AkachainMember\\Check_Maphieu\"\n",
    "file_list = glob.glob(dir + \"/ICT_transaction*.xlsx\")\n",
    "\n",
    "excl_list = []\n",
    "\n",
    "for file in file_list:\n",
    "    excl_list.append(pd.read_excel(file))\n",
    "# create a new dataframe to store the\n",
    "# merged excel file.\n",
    "excl_merged = pd.DataFrame()\n",
    " \n",
    "for excl_file in excl_list:\n",
    "    # appends the data into the excl_merged\n",
    "    # dataframe.\n",
    "    excl_merged = excl_merged.append(\n",
    "      excl_file, ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Do các giao dịch Rollback ghi nhận điểm sai nên cần itn1h lại\n",
    "excl_merged['Point_cal'] = np.where(excl_merged['Mã giao dịch'].str.startswith('Rollback',na=False), excl_merged['Điểm xếp hạng'], excl_merged['Điểm đổi quà'])\n",
    "excl_merged[excl_merged['Mã giao dịch'].str.startswith('Rollback',na=False)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = excl_merged.groupby('Mã thành viên', as_index=False).agg(totalP = ('Point_cal','sum'))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x.totalP.sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "excl_merged"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x[x['Mã thành viên'] == '01378e1d-e3d0-499b-9492-ed93198516fd']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "excl_merged[excl_merged['Mã thành viên']=='01378e1d-e3d0-499b-9492-ed93198516fd'].sort_values(by = 'Ngày giao dịch', ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "list_roll_cus = excl_merged[excl_merged['Mã giao dịch'].str.startswith('Rollback',na=False)]['Mã thành viên'].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Export balance cho anh Chương\n",
    "data_ex = excl_merged[['Mã thành viên', 'Mã giao dịch','Mã giao dịch gốc', 'Điểm xếp hạng', 'Điểm đổi quà', 'Tổng số tiền tính toán', 'Ngày tạo']]\n",
    "data_ex['Ngày tạo'] = data_ex['Ngày tạo'].dt.date\n",
    "data_ex['Count_ex'] = 1\n",
    "data_ex['Mã giao dịch'] = data_ex['Mã giao dịch'].astype(str)\n",
    "data_ex = data_ex[data_ex['Ngày tạo'] < dt.date(2023,1,1)]\n",
    "data_ex['Point_cal'] = np.where(data_ex['Mã giao dịch'].str.startswith('Rollback',na=False), data_ex['Điểm xếp hạng'], data_ex['Điểm đổi quà'])\n",
    "\n",
    "data_mir = pd.read_sql(\"\"\"SELECT [Mã thành viên], [Lý do], [Mã giao dịch], [Số tiền mua], [Điểm phát sinh], [Ngày tạo] FROM F_ICT_AKACHAIN_LOYALTY WHERE [Ngày tạo] < '2023-01-01' AND [Lý do] = 'Migration' \"\"\", getEngine(SaleMart))\n",
    "data_mir = data_mir[data_mir['Điểm phát sinh']!=0]\n",
    "data_mir = data_mir.groupby('Mã thành viên', as_index=False).agg(totalMir = ('Điểm phát sinh', 'sum'))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = data_ex.groupby('Mã thành viên', as_index=False).agg(totalPoint = ('Point_cal','sum'))\n",
    "x = x.merge(data_mir[['Mã thành viên','totalMir']], on='Mã thành viên', how='outer')\n",
    "x.fillna(0, inplace=True)\n",
    "x['FinalPoint'] = x['totalPoint'] + x['totalMir']\n",
    "# x.to_excel(dir + \"/Check.xlsx\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x.FinalPoint.sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x[x.FinalPoint != 0 ].to_excel(dir + \"/Exp.230210.xlsx\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x[x['Mã thành viên'] == '01378e1d-e3d0-499b-9492-ed93198516fd']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "excl_merged[excl_merged['Mã thành viên']=='4d6fa515-02c5-4c95-96b3-e1af92f900fb']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Export lại Data\n",
    "dir  = r\"C:\\Users\\Admin\\OneDrive\\FRT\\6. Stored Data\\AkachainMember\\Export Balance 221231\"\n",
    "\n",
    "file_list = glob.glob(dir + \"/22_*.xlsx\")\n",
    "\n",
    "excl_list = []\n",
    "\n",
    "for file in file_list:\n",
    "    excl_list.append(pd.read_excel(file))\n",
    "# create a new dataframe to store the\n",
    "# merged excel file.\n",
    "excl_merged = pd.DataFrame()\n",
    " \n",
    "for excl_file in excl_list:\n",
    "    # appends the data into the excl_merged\n",
    "    # dataframe.\n",
    "    excl_merged = excl_merged.append(\n",
    "      excl_file, ignore_index=True)\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "excl_merged.to_csv(dir + \"/OutputMerge_2022.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_merge = pd.read_csv(dir + \"/OutputMerge_2022.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_query =  \"\"\"  SELECT [Ngày tạo], [Lý do], [Mã giao dịch], [Điểm phát sinh]\n",
    "  FROM [FLC_SHOP_SALES_DATAWAREHOUSE].[dbo].[F_ICT_AKACHAIN_LOYALTY]\n",
    "  WHERE [Ngày tạo] < '2023-01-01' \"\"\"\n",
    "df_34 = pd.read_sql(_query, getEngine(SaleMart))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Ngày 20/03/2023: Danh  request cách đối chiếu giữa 34 và az"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f_date = '2023-02-01'\n",
    "t_date = '2023-02-28'\n",
    "import polars as pl \n",
    "\n",
    "\n",
    "df_34 = pd.read_sql(f\"\"\"SELECT \n",
    "                        [Mã thành viên] AS 'MEMBER_CODE_AKC',\n",
    "                        [Ngày tạo] AS 'CREATED_DATE',\n",
    "                        [Ngày giao dịch] AS 'TRANSACTION_DATE',\n",
    "                        [Lý do] AS 'POINT_INFORMATION',\n",
    "                        [Mã giao dịch] AS 'DOCENTRY',\n",
    "                        [Số tiền mua] AS 'AMOUNT',\n",
    "                        [Điểm xếp hạng quy đổi] AS 'POINT_RANKING',\n",
    "                        [Điểm phát sinh] AS 'POINT'\n",
    "                        FROM F_ICT_AKACHAIN_LOYALTY \n",
    "                        WHERE [Ngày giao dịch] BETWEEN '{f_date}' AND '{t_date}' \"\"\",  getEngine(SaleMart))\n",
    "df_34= df_34.rename(columns=str.lower)\n",
    "df_34.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_az = pd.read_sql(f\"\"\"SELECT  *\n",
    "FROM [dbo].[VF_CID_AKC_TRANSACTION_LOYALTY_ICT]\n",
    "WHERE CAST(CREATED_DATE AS DATE)  BETWEEN '{f_date}' AND '{t_date}' \"\"\",  azr_engine)\n",
    "df_az= df_az.rename(columns=str.lower)\n",
    "\n",
    "df_az.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Diff Inv\n",
    "inv_34  = df_34[df_34['point_information'] == 'Invoice']\n",
    "inv_34 = inv_34.groupby(['member_code_akc', 'docentry']).agg(\n",
    "    point_34 = ('point','sum'),\n",
    "    amt_34 = ('amount', 'sum')\n",
    ")\n",
    "inv_az  = df_az[df_az['point_information'] == 'Invoice']\n",
    "inv_az = inv_az.rename(columns={'invoice_header':'docentry'}).groupby(['member_code_akc', 'docentry']).agg(\n",
    "    point_az = ('invoice_point','sum'),\n",
    "    amt_az = ('list_amount_af_vat', 'sum')\n",
    ")\n",
    "display(inv_34.head(3))\n",
    "display(inv_az.head(3))\n",
    "print(inv_az.count(), inv_az.count())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "merge_inv = inv_34.merge(inv_az, how='outer', left_index=True, right_index=True)\n",
    "# merge_inv.fillna(0, inplace=True)\n",
    "merge_inv['diff_point'] = merge_inv.point_34.fillna(0) - merge_inv.point_az.fillna(0)\n",
    "merge_inv['diff_amt'] = merge_inv.amt_34.fillna(0) - merge_inv.amt_az.fillna(0)\n",
    "merge_inv['diff_point_flg'] = np.where(merge_inv.point_az != merge_inv.point_34 , True,False)\n",
    "merge_inv['diff_amt_flg'] = np.where(merge_inv.amt_34 !=  merge_inv.amt_az , True,False)\n",
    "\n",
    "print(merge_inv[['diff_point_flg', 'diff_amt_flg']].sum())\n",
    "print(merge_inv.diff_point.abs().sum() )\n",
    "print(merge_inv.diff_amt.abs().sum() )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "merge_inv[merge_inv.diff_point_flg==True]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "merge_inv[(merge_inv.diff_amt_flg==True) & (merge_inv.diff_point_flg==False)].sort_values('diff_amt',ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 23/03/2023: Export chạy thập phân vị cho a chương số điểm lũy kế từng khách\n",
    "from HelperMaster import cal_percentile\n",
    "df = pd.read_csv(r'C:\\Users\\Admin\\OneDrive\\FRT\\6. Stored Data\\AkachainMember\\230323_export.csv')\n",
    "df.drop(columns='IsGrandTotalRowTotal', inplace=True)\n",
    "df = df[~df['Mã thành viên'].isna()]\n",
    "df = df[df['Số_điểm_Lũy_kế']!=0]\n",
    "\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[>7890.0.sort_values(by = 'Số_điểm_Lũy_kế', ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# \n",
    "df_greater = df[df['Số_điểm_Lũy_kế']>0]\n",
    "df_perc = cal_percentile(df_greater['Số_điểm_Lũy_kế'])\n",
    "print('total Cus =', len(df_greater))\n",
    "df_greater['perc'] = np.where((df_greater['Số_điểm_Lũy_kế'] >= 2) & (df_greater['Số_điểm_Lũy_kế'] < 40), 10,\n",
    "                        np.where((df_greater['Số_điểm_Lũy_kế'] >= 40) & (df_greater['Số_điểm_Lũy_kế'] < 70), 20,\n",
    "                        np.where((df_greater['Số_điểm_Lũy_kế'] >= 70) & (df_greater['Số_điểm_Lũy_kế'] < 130), 30,\n",
    "                        np.where((df_greater['Số_điểm_Lũy_kế'] >= 130) & (df_greater['Số_điểm_Lũy_kế'] < 240), 40,\n",
    "                        np.where((df_greater['Số_điểm_Lũy_kế'] >= 240) & (df_greater['Số_điểm_Lũy_kế'] < 630), 50,\n",
    "                        np.where((df_greater['Số_điểm_Lũy_kế'] >= 630) & (df_greater['Số_điểm_Lũy_kế'] < 1200), 60,\n",
    "                        np.where((df_greater['Số_điểm_Lũy_kế'] >= 1200) & (df_greater['Số_điểm_Lũy_kế'] < 2340), 70,\n",
    "                        np.where((df_greater['Số_điểm_Lũy_kế'] >= 2340) & (df_greater['Số_điểm_Lũy_kế'] < 4430), 80,\n",
    "                        np.where((df_greater['Số_điểm_Lũy_kế'] >= 4430) & (df_greater['Số_điểm_Lũy_kế'] < 7890), 90,\n",
    "                        np.where((df_greater['Số_điểm_Lũy_kế'] >= 7890) & (df_greater['Số_điểm_Lũy_kế'] <= 1073444), 100,\n",
    "                        None))))))))))\n",
    "display(df_perc)\n",
    "display(df_greater.groupby('perc', as_index=False).agg(\n",
    "    total_cus = ('Mã thành viên','nunique'),\n",
    "    total_point = ('Số_điểm_Lũy_kế', 'sum')\n",
    "))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_greater[df_greater.perc >= 90].to_excel(r\"C:\\Users\\Admin\\OneDrive\\FRT\\6. Stored Data\\AkachainMember\\cus_balance_190323.xlsx\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# \n",
    "df_less = df[df['Số_điểm_Lũy_kế']<0]\n",
    "df_perc = cal_percentile(df_less['Số_điểm_Lũy_kế'])\n",
    "print('total Cus =', len(df_less))\n",
    "df_less['perc'] = np.where((df_less['Số_điểm_Lũy_kế'] >= -194546.0) & (df_less['Số_điểm_Lũy_kế'] < -7290.0), 10,\n",
    "                        np.where((df_less['Số_điểm_Lũy_kế'] >= -7290.0) & (df_less['Số_điểm_Lũy_kế'] < -4948.0), 20,\n",
    "                        np.where((df_less['Số_điểm_Lũy_kế'] >= -4948.0) & (df_less['Số_điểm_Lũy_kế'] < -3840.0), 30,\n",
    "                        np.where((df_less['Số_điểm_Lũy_kế'] >= -3840.0) & (df_less['Số_điểm_Lũy_kế'] < -3160.0), 40,\n",
    "                        np.where((df_less['Số_điểm_Lũy_kế'] >= -3160.0) & (df_less['Số_điểm_Lũy_kế'] < -2650.0), 50,\n",
    "                        np.where((df_less['Số_điểm_Lũy_kế'] >= -2650.0) & (df_less['Số_điểm_Lũy_kế'] < -2090.0), 60,\n",
    "                        np.where((df_less['Số_điểm_Lũy_kế'] >= -2090.0) & (df_less['Số_điểm_Lũy_kế'] < -1490.0), 70,\n",
    "                        np.where((df_less['Số_điểm_Lũy_kế'] >= -1490.0) & (df_less['Số_điểm_Lũy_kế'] < -738.0), 80,\n",
    "                        np.where((df_less['Số_điểm_Lũy_kế'] >= -738.0) & (df_less['Số_điểm_Lũy_kế'] < -230.0), 90,\n",
    "                        np.where((df_less['Số_điểm_Lũy_kế'] >= -230.0) & (df_less['Số_điểm_Lũy_kế'] <= -10.0), 100,\n",
    "                        None))))))))))\n",
    "display(df_perc)\n",
    "display(df_less.groupby('perc', as_index=False).agg(\n",
    "    total_cus = ('Mã thành viên','nunique'),\n",
    "    total_point = ('Số_điểm_Lũy_kế', 'sum')\n",
    "))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "display(df[df['Số_điểm_Lũy_kế']<0].sort_values(by='Số_điểm_Lũy_kế'))\n",
    "cal_percentile(df[df['Số_điểm_Lũy_kế']<0]['Số_điểm_Lũy_kế'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 19/05/2023 : Export percentile again\n",
    "import pandas as pd\n",
    "from HelperMaster import cal_percentile\n",
    "import numpy as np\n",
    "df_sql = pd.read_csv(r\"C:\\Users\\Admin\\OneDrive\\FRT\\6. Stored Data\\AkachainMember\\Data_20230519_Percentile_Balance\\Data_Export_SQL.csv\", )\n",
    "df_sql.rename(columns={'(No column name)':'total_point_sql'}, inplace=True)\n",
    "df_pbi = pd.read_csv(r\"C:\\Users\\Admin\\OneDrive\\FRT\\6. Stored Data\\AkachainMember\\Data_20230519_Percentile_Balance\\Data_Export_PBI.csv\", )\n",
    "df_pbi = df_pbi[df_pbi['IsGrandTotalRowTotal'] == False] \n",
    "df_pbi.drop(columns=('IsGrandTotalRowTotal'), inplace=True)\n",
    "\n",
    "# Cẩn thận khi export từ sql, phải lấy type net = 0\n",
    "\n",
    "df_merge = df_sql.merge(df_pbi, how='outer', on = 'Mã thành viên')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_pbi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_greater = df_pbi[df_pbi['Số_điểm_Lũy_kế']>0]\n",
    "df_perc = cal_percentile(df_greater['Số_điểm_Lũy_kế'])\n",
    "list_perc_val = df_perc['Value'].values\n",
    "print('total Cus =', len(df_greater))\n",
    "df_greater['perc'] = np.where((df_greater['Số_điểm_Lũy_kế'] >= list_perc_val[0]) & (df_greater['Số_điểm_Lũy_kế'] < list_perc_val[1]), 10,\n",
    "                        np.where((df_greater['Số_điểm_Lũy_kế'] >= list_perc_val[1]) & (df_greater['Số_điểm_Lũy_kế'] < list_perc_val[2]), 20,\n",
    "                        np.where((df_greater['Số_điểm_Lũy_kế'] >= list_perc_val[2]) & (df_greater['Số_điểm_Lũy_kế'] < list_perc_val[3]), 30,\n",
    "                        np.where((df_greater['Số_điểm_Lũy_kế'] >= list_perc_val[3]) & (df_greater['Số_điểm_Lũy_kế'] < list_perc_val[4]), 40,\n",
    "                        np.where((df_greater['Số_điểm_Lũy_kế'] >= list_perc_val[4]) & (df_greater['Số_điểm_Lũy_kế'] < list_perc_val[5]), 50,\n",
    "                        np.where((df_greater['Số_điểm_Lũy_kế'] >= list_perc_val[5]) & (df_greater['Số_điểm_Lũy_kế'] < list_perc_val[6]), 60,\n",
    "                        np.where((df_greater['Số_điểm_Lũy_kế'] >= list_perc_val[6]) & (df_greater['Số_điểm_Lũy_kế'] < list_perc_val[7]), 70,\n",
    "                        np.where((df_greater['Số_điểm_Lũy_kế'] >= list_perc_val[7]) & (df_greater['Số_điểm_Lũy_kế'] < list_perc_val[8]), 80,\n",
    "                        np.where((df_greater['Số_điểm_Lũy_kế'] >= list_perc_val[8]) & (df_greater['Số_điểm_Lũy_kế'] < list_perc_val[9]), 90,\n",
    "                        np.where((df_greater['Số_điểm_Lũy_kế'] >= list_perc_val[9]) & (df_greater['Số_điểm_Lũy_kế'] <= list_perc_val[10]), 100,\n",
    "                        None))))))))))\n",
    "display(df_perc)\n",
    "display(df_greater.groupby('perc', as_index=False).agg(\n",
    "    total_cus = ('Mã thành viên','nunique'),\n",
    "    total_point = ('Số_điểm_Lũy_kế', 'sum')\n",
    "))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_greater[df_greater['perc']>=90].to_excel(r\"C:\\Users\\Admin\\OneDrive\\FRT\\6. Stored Data\\AkachainMember\\Data_20230519_Percentile_Balance\\Top_20_percentile.xlsx\",index=False )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(df_greater[df_greater['Số_điểm_Lũy_kế'] >=15857])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tính thêm từ 90-100\n",
    "def cal_percentile_m(df_):\n",
    "    _range = np.arange(90, 101, 1)\n",
    "    _percentile = np.percentile(df_, _range)\n",
    "    df_return = pd.DataFrame({'Perc': _range, 'Value': _percentile})\n",
    "    return df_return\n",
    "df_perc_90 = cal_percentile_m(df_pbi[df_pbi['Số_điểm_Lũy_kế']>0]['Số_điểm_Lũy_kế'])\n",
    "list_perc_val = df_perc_90['Value'].values\n",
    "list_perc_ind = df_perc_90['Perc'].values\n",
    "\n",
    "\n",
    "df_greater_90 = df_greater.copy()\n",
    "df_greater_90['perc'] = np.where((df_greater_90['Số_điểm_Lũy_kế'] >= list_perc_val[0]) & (df_greater['Số_điểm_Lũy_kế'] < list_perc_val[1]), list_perc_ind[0],\n",
    "                        np.where((df_greater_90['Số_điểm_Lũy_kế'] >= list_perc_val[1]) & (df_greater['Số_điểm_Lũy_kế'] < list_perc_val[2]), list_perc_ind[1],\n",
    "                        np.where((df_greater_90['Số_điểm_Lũy_kế'] >= list_perc_val[2]) & (df_greater['Số_điểm_Lũy_kế'] < list_perc_val[3]), list_perc_ind[2],\n",
    "                        np.where((df_greater_90['Số_điểm_Lũy_kế'] >= list_perc_val[3]) & (df_greater_90['Số_điểm_Lũy_kế'] < list_perc_val[4]), list_perc_ind[3],\n",
    "                        np.where((df_greater_90['Số_điểm_Lũy_kế'] >= list_perc_val[4]) & (df_greater_90['Số_điểm_Lũy_kế'] < list_perc_val[5]), list_perc_ind[4],\n",
    "                        np.where((df_greater_90['Số_điểm_Lũy_kế'] >= list_perc_val[5]) & (df_greater_90['Số_điểm_Lũy_kế'] < list_perc_val[6]), list_perc_ind[5],\n",
    "                        np.where((df_greater_90['Số_điểm_Lũy_kế'] >= list_perc_val[6]) & (df_greater_90['Số_điểm_Lũy_kế'] < list_perc_val[7]), list_perc_ind[6],\n",
    "                        np.where((df_greater_90['Số_điểm_Lũy_kế'] >= list_perc_val[7]) & (df_greater_90['Số_điểm_Lũy_kế'] < list_perc_val[8]), list_perc_ind[7],\n",
    "                        np.where((df_greater_90['Số_điểm_Lũy_kế'] >= list_perc_val[8]) & (df_greater_90['Số_điểm_Lũy_kế'] < list_perc_val[9]), list_perc_ind[8],\n",
    "                        np.where((df_greater_90['Số_điểm_Lũy_kế'] >= list_perc_val[9]) & (df_greater_90['Số_điểm_Lũy_kế'] <= list_perc_val[10]), list_perc_ind[9],\n",
    "                        None))))))))))\n",
    "display(df_perc_90)\n",
    "display(df_greater_90.groupby('perc', as_index=False).agg(\n",
    "    total_cus = ('Mã thành viên','nunique'),\n",
    "    total_point = ('Số_điểm_Lũy_kế', 'sum')\n",
    "))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "list_perc_ind"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_lesser = df_pbi[df_pbi['Số_điểm_Lũy_kế']<0]\n",
    "df_perc_lesser = cal_percentile(df_lesser['Số_điểm_Lũy_kế'])\n",
    "list_perc_val_lesser = df_perc_lesser['Value'].values\n",
    "print('total Cus =', len(df_lesser))\n",
    "df_lesser['perc'] = np.where((df_lesser['Số_điểm_Lũy_kế'] >= list_perc_val_lesser[0]) & (df_lesser['Số_điểm_Lũy_kế'] < list_perc_val_lesser[1]), 10,\n",
    "                        np.where((df_lesser['Số_điểm_Lũy_kế'] >= list_perc_val_lesser[1]) & (df_lesser['Số_điểm_Lũy_kế'] < list_perc_val_lesser[2]), 20,\n",
    "                        np.where((df_lesser['Số_điểm_Lũy_kế'] >= list_perc_val_lesser[2]) & (df_lesser['Số_điểm_Lũy_kế'] < list_perc_val_lesser[3]), 30,\n",
    "                        np.where((df_lesser['Số_điểm_Lũy_kế'] >= list_perc_val_lesser[3]) & (df_lesser['Số_điểm_Lũy_kế'] < list_perc_val_lesser[4]), 40,\n",
    "                        np.where((df_lesser['Số_điểm_Lũy_kế'] >= list_perc_val_lesser[4]) & (df_lesser['Số_điểm_Lũy_kế'] < list_perc_val_lesser[5]), 50,\n",
    "                        np.where((df_lesser['Số_điểm_Lũy_kế'] >= list_perc_val_lesser[5]) & (df_lesser['Số_điểm_Lũy_kế'] < list_perc_val_lesser[6]), 60,\n",
    "                        np.where((df_lesser['Số_điểm_Lũy_kế'] >= list_perc_val_lesser[6]) & (df_lesser['Số_điểm_Lũy_kế'] < list_perc_val_lesser[7]), 70,\n",
    "                        np.where((df_lesser['Số_điểm_Lũy_kế'] >= list_perc_val_lesser[7]) & (df_lesser['Số_điểm_Lũy_kế'] < list_perc_val_lesser[8]), 80,\n",
    "                        np.where((df_lesser['Số_điểm_Lũy_kế'] >= list_perc_val_lesser[8]) & (df_lesser['Số_điểm_Lũy_kế'] < list_perc_val_lesser[9]), 90,\n",
    "                        np.where((df_lesser['Số_điểm_Lũy_kế'] >= list_perc_val_lesser[9]) & (df_lesser['Số_điểm_Lũy_kế'] <= list_perc_val_lesser[10]), 100,\n",
    "                        None))))))))))\n",
    "display(df_perc_lesser)\n",
    "display(df_lesser.groupby('perc', as_index=False).agg(\n",
    "    total_cus = ('Mã thành viên','nunique'),\n",
    "    total_point = ('Số_điểm_Lũy_kế', 'sum')\n",
    "))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_sql.rename(columns={'(No column name)':'total_point_sql'},)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_sql"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 24/03/2023\n",
    "# Danh check lỗi thiếu doc\n",
    "dir = r'C:\\Users\\Admin\\OneDrive\\FRT\\6. Stored Data\\AkachainMember\\Check_Batthuong_230324'\n",
    "df_thang_1 = pd.read_excel(dir + \"/Thang1.xlsx\", dtype={'Mã giao dịch':'object'})\n",
    "df_thang_2 = pd.read_excel(dir + \"/Thang2.xlsx\", dtype={'Mã giao dịch':'object'})\n",
    "df_thang_3 = pd.read_excel(dir + \"/Thang3.xlsx\", dtype={'Mã giao dịch':'object'})\n",
    "df_dw = pd.read_csv( dir + '/DB.csv', dtype={'Mã giao dịch':'object'})\n",
    "df_merge = pd.concat([df_thang_1, df_thang_2, df_thang_3])\n",
    "# df_merge.drop(columns=['Điểm đổi quà đã sử dụng'], inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_join =  df_merge.merge(df_dw, on= ['Lý do', 'Mã giao dịch'], how='left', suffixes=['_export', '_dw'], indicator=True)\n",
    "df_join.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_join[df_join['Mã giao dịch'] == '1080100']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_join.groupby('_merge').agg(totalL = ('_merge', 'count'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_join[df_join['_merge'] == 'left_only'].sort_values(by=['Ngày giao dịch', 'Mã giao dịch'] ).to_excel(dir + \"/output_check.xlsx\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 28/03/2023: Check xem có ngày nào bị full disk nữa không\n",
    "#\n",
    "date_from  = '2023-03-01' \n",
    "date_to = '2023-03-20'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_dw = pd.read_sql(f\"SELECT * FROM [F_ICT_AKACHAIN_LOYALTY] WHERE [Ngày giao dịch] BETWEEN '{date_from}' AND '{date_to}' \", getEngine(SaleMart))\n",
    "# raw_dw = raw_dw.drop('Điểm đổi quà đã sử dụng', inplace=True)\n",
    "raw_dw.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "query_az = f\"\"\"SELECT  * \n",
    "            FROM [VF_CID_AKC_TRANSACTION_LOYALTY_ICT] \n",
    "            WHERE DATE_KEY \n",
    "            BETWEEN {date_from.replace('-','')} AND {date_to.replace('-','')}\n",
    "            \"\"\"\n",
    "raw_az = pd.read_sql(query_az, azr_engine)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_az = raw_az.copy()\n",
    "df_az['TRANSACTION_STATUS'] = df_az.TRANSACTION_STATUS.str.strip()\n",
    "df_az['docentry_mapping'] = df_az.INVOICE_HEADER\n",
    "\n",
    "\n",
    "df_dw = raw_dw.copy()\n",
    "\n",
    "\n",
    "df_dw['Lý do'] = np.where(df_dw['Lý do'] == 'Burning', 'Use Point'\n",
    "                , np.where(df_dw['Lý do'] == 'InvoiceReturnFull', 'ReturnFull'\n",
    "                , np.where(df_dw['Lý do'] == 'InvoiceReturnPartial', 'ReturnPartial'\n",
    "\n",
    "                , df_dw['Lý do'])))\n",
    "df_dw['docentry_mapping'] = df_dw.apply(lambda x : x['Mã giao dịch'].rsplit('_',1)[1] if x['Lý do'] == 'Use Point' else x['Mã giao dịch'], axis = 1)\n",
    "# df_dw[df_dw['Mã thành viên'] == '0027bd9a-db1e-42a9-b2ae-b2f53f65d70b']\n",
    "\n",
    "# Nếu Partner Refund có use point thì là netoff\n",
    "df_refund = df_dw[df_dw['Lý do'] == 'PartnerRefund'][['Mã thành viên', 'docentry_mapping','Điểm phát sinh']]\n",
    "df_refund['Điểm phát sinh'] = -df_refund['Điểm phát sinh']\n",
    "df_refund = df_refund.merge(df_dw[df_dw['Lý do']!= 'Invoice'], on = ['Mã thành viên', 'docentry_mapping','Điểm phát sinh'], how = 'inner')\n",
    "doc_refund = df_refund.docentry_mapping.values\n",
    "\n",
    "df_dw = df_dw[~df_dw.docentry_mapping.isin(doc_refund)]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_dw[df_dw.docentry_mapping.isin(doc_refund)]['Điểm phát sinh'].sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(df_dw), len(raw_dw)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_dw['Lý do'].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_az.TRANSACTION_STATUS.unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(df_dw), len(df_az)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "group_az = df_az.groupby(['MEMBER_CODE_AKC', 'POINT_INFORMATION', 'docentry_mapping','TRANSACTION_STATUS'], as_index=False).agg(\n",
    "    total_point_az = ('INVOICE_POINT', 'sum'),\n",
    ")\n",
    "group_dw = df_dw.groupby(['Mã thành viên', 'Lý do', 'docentry_mapping'], as_index=False).agg(\n",
    "    total_point_dw = ('Điểm phát sinh', 'sum')\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "group_dw.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "group_az.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "merge = group_az.merge(group_dw, \n",
    "                       left_on=['MEMBER_CODE_AKC', 'POINT_INFORMATION', 'docentry_mapping'],\n",
    "                        right_on=['Mã thành viên', 'Lý do', 'docentry_mapping'],\n",
    "                       how = 'outer',indicator=True)\n",
    "merge['diff_point'] = np.where((merge._merge == 'both') & (merge.POINT_INFORMATION == 'Use Point'), merge.total_point_az.fillna(0) + merge.total_point_dw.fillna(0)\n",
    "                    ,merge.total_point_az.fillna(0) - merge.total_point_dw.fillna(0)\n",
    ")\n",
    "merge.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "merge.groupby('_merge').count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Both : Total = không diff\n",
    "merge[merge._merge == 'both']['diff_point'].sum()\n",
    "# merge[merge._merge == 'both'][merge.diff_point!=0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Id gì\n",
    "\n",
    "merge[(merge._merge == 'both') & (merge.TRANSACTION_STATUS != 'F')].head(3)\n",
    "# merge.TRANSACTION_STATUS.unique()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "display(merge[merge._merge ==  'left_only'].groupby('POINT_INFORMATION').docentry_mapping.count())\n",
    "\n",
    "\n",
    "display(merge[merge._merge ==  'right_only'].groupby('Lý do').docentry_mapping.count())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "merge[merge._merge ==  'left_only'].groupby('POINT_INFORMATION').docentry_mapping.min(),merge[merge._merge ==  'left_only'].groupby('POINT_INFORMATION').docentry_mapping.max()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "merge[merge._merge ==  'left_only'].groupby(['POINT_INFORMATION', 'TRANSACTION_STATUS']).count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "merge[merge._merge ==  'right_only'].groupby('Lý do').count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "merge[(merge._merge ==  'right_only') & (merge['Lý do'] == 'Use Point')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "merge[(merge._merge ==  'right_only') & (merge['Lý do'] == 'Invoice')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hiện tại chỉ có trường hợp là\n",
    "# "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mem_check = 'ebd7fc47-b89a-4c56-b0f4-46af7dddd8a1'\n",
    "merge[(merge.MEMBER_CODE_AKC == mem_check)|(merge['Mã thành viên'] ==mem_check)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Request 04/04: AB Testing\n",
    "\n",
    "4 nhóm mỗi nhóm 2_000 khách có acumulate totalPoint > 5000\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 04/04/2023\n",
    "# Request: 3 nhóm mỗi nhóm 2_000 khách có acumulate totalPoint > 5000\n",
    "import pandas as pd\n",
    "from HelperMaster import cal_percentile\n",
    "# dir = r'C:\\Users\\Admin\\OneDrive\\FRT\\6. Stored Data\\AkachainMember\\AB_Testing_230404'\n",
    "dir = r'C:\\Users\\Admin\\OneDrive\\FRT\\6. Stored Data\\AkachainMember\\AB_Testing_230404\\Round 2_230419'\n",
    "\n",
    "# Dữ liệu point được akc verify\n",
    "point_of_akc = pd.read_excel(dir + r\"/Data_AKC_Verify.xlsx\")\n",
    "\n",
    "# Data point gen từ 34\n",
    "df = pd.read_csv( dir + \"/List_Export.csv\")\n",
    "df = df.rename(columns={\"Mã thành viên\": \"id\", \"Monetary\":\"Monetary_akc\"})\n",
    "df = df.merge(point_of_akc[['Mã thành viên', 'Fgold', 'phone_number']], left_on= 'id', right_on='Mã thành viên', how = 'left')\n",
    "df.drop(columns=['Mã thành viên', 'totalPoint'], inplace=True)\n",
    "df = df.rename(columns={'Fgold':'totalPoint'})\n",
    "df = df[df.Recency < 120]\n",
    "df = df[df.totalPoint < 100_000]\n",
    "df = df[df.totalPoint > 5_000]\n",
    "df = df[df.phone_number.str[:2] =='84']\n",
    "\n",
    "df_length_base = len(df)\n",
    "print(f'Length of df {len(df):,.0f}')\n",
    "display(df.head(3))\n",
    "\n",
    "# Read data azure\n",
    "df_mm = pd.read_csv(dir + \"/data_mm.csv\") # Data min max , trong câu SQL số 1\n",
    "df_sale_agg = pd.read_csv(dir + \"/data_sale_agg.csv\") # Data Sale agg trong câu SQL số 2\n",
    "df_sale_agg.fillna(0, inplace=True)\n",
    "\n",
    "df = df.merge(df_mm, on='CUSTOMER_KEY', how ='inner')\n",
    "print(f\"Length after join mm: {len(df):,.0f}, minus: {df_length_base - len(df)}\" )\n",
    "\n",
    "df = df.merge(df_sale_agg, on='CUSTOMER_KEY', how ='inner')\n",
    "print(f\"Length after join sale_agg :{len(df):,.0f}, minus: {df_length_base - len(df)}\" )\n",
    "\n",
    "# # Calculate\n",
    "df['two_buy_gap'] = df.MIN_MAX_BUY / df.TOTAL_BUY\n",
    "df['AMT_APPLE_PERCENT'] = df.TOTAL_AMT_APPLE / df.TOTAL_AMT\n",
    "df['AMT_PKNK_PERCENT'] = df.TOTAL_AMT_PKNK / df.TOTAL_AMT\n",
    "df['AMT_DTDD_OTHER_PERCENT'] = df.TOTAL_AMT_DTDD_OTHER / df.TOTAL_AMT\n",
    "df['AMT_PC_PERCENT'] = df.TOTAL_AMT_PC / df.TOTAL_AMT\n",
    "df.columns = map(str.lower, df.columns)\n",
    "\n",
    "df = df[df.total_amt.between(1_000_000, 500_000_000)]\n",
    "\n",
    "display(df.head(3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cal_percentile(df['total_amt'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xx = (np.round(df['total_amt']/1_000_000,0))\n",
    "from matplotlib import pyplot as plt\n",
    "\n",
    "plt.hist(xx, bins=100, alpha=0.5, log=True)\n",
    "# plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define columns to use \n",
    "df_cal = df.copy()\n",
    "att_col = [\n",
    "    'totalpoint', 'recency','monetary_akc',\n",
    "    'total_amt','two_buy_gap', \n",
    "    'total_buy',\n",
    "    'total_amt_apple',\n",
    "    'amt_apple_percent', ]\n",
    "\n",
    "target_col = 'id'\n",
    "n_cut = 4\n",
    "\n",
    "# Using bin and qcut:\n",
    "for col_ in att_col:\n",
    "    print(col_)\n",
    "    if col_ in (['frequency','amt_pknk_percent', 'amt_dtdd_other_percent','amt_pc_percent', 'total_buy']):\n",
    "        df_cal[f'{col_}_bins'] = pd.qcut(df_cal[col_].rank(method='first'), q=n_cut, labels=[i for i in range(1, n_cut +1)])\n",
    "    else:\n",
    "        df_cal[f'{col_}_bins'] = pd.qcut(df_cal[col_], q=n_cut, labels=[i for i in range(1, n_cut +1)])\n",
    "\n",
    "bins_col = [c for c in df_cal.columns if '_bins' in c]\n",
    "df_cal['bin_combine'] = df_cal[bins_col].apply(\n",
    "    lambda row: '_'.join(row.values.astype(str)), axis=1)\n",
    "\n",
    "dist_of_bins = df_cal.groupby('bin_combine', as_index=False).agg(count_records = ('bin_combine', 'count'))\n",
    "dist_of_bins['percent'] = dist_of_bins.count_records / dist_of_bins.count_records.sum()\n",
    "\n",
    "n_record_treshold = 30\n",
    "dist_of_bins = dist_of_bins[dist_of_bins.count_records > n_record_treshold]\n",
    "\n",
    "print(f'Total bins: {df_cal.bin_combine.nunique()}, list bins > {n_record_treshold}: {len(dist_of_bins)}' )\n",
    "\n",
    "#  List bins\n",
    "list_bins_combine = dist_of_bins.bin_combine.unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tính percent get version 2: Dynamic\n",
    "N_GROUP = 4\n",
    "TARGET = 2_000 * N_GROUP * 1.78\n",
    "PERCENT_GET = TARGET / len(df_cal)\n",
    "\n",
    "df_bins_label = pd.DataFrame()\n",
    "for b in list_bins_combine: \n",
    "    # if b == '1_1_1_1_1_3_1_1':\n",
    "        df_b = df_cal[df_cal.bin_combine == b]\n",
    "        # Random DataFrame\n",
    "        df_b = df_cal[df_cal.bin_combine == b]\n",
    "        df_random = df_b.sample(frac=PERCENT_GET,)\n",
    "        # Chia thành 3 nhóm\n",
    "        group_size = df_random.shape[0] // N_GROUP\n",
    "        du = df_random.shape[0] % N_GROUP\n",
    "        groups = {}\n",
    "        for n in range(N_GROUP):\n",
    "            groups[n] = df_random.iloc[group_size * n : group_size * (n+1)].assign(flag=f'group{n}')\n",
    "            \n",
    "        # Gộp lại thành DataFrame mới\n",
    "        df_labeled = pd.concat([groups[i] for i in range(N_GROUP)], axis=0)\n",
    "        # print(len(df_labeled))\n",
    "        df_bins_label = pd.concat([df_bins_label, df_labeled])\n",
    "\n",
    "print(f\"Length of result:  {len(df_bins_label):,.0f}\")\n",
    "print(f\"Length of origin:  {len(df_cal):,.0f}\")\n",
    "print(f\"Percent of result: {len(df_bins_label) / len(df_cal) * 100:,.3f}%\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_bins_label.groupby(['flag'], as_index=False).agg(countD = ('flag','count'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pair plot of distributions\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "pair_col = att_col.copy()\n",
    "pair_col.append('flag')\n",
    "g = sns.pairplot(df_bins_label[pair_col],\n",
    "                 height=1.8, \n",
    "                 corner=False,\n",
    "                 plot_kws={'alpha':0.3},\n",
    "                 hue='flag',\n",
    "                 kind='kde',\n",
    "                )\n",
    "# plt.show()\n",
    "log_columns = [\"total_amt\", \"total_amt_apple\", 'monetary_akc', 'total_buy', ]\n",
    "\n",
    "# for ax in g.axes.flat:\n",
    "#     if ax.get_xlabel() in log_columns:\n",
    "#         ax.set(xscale=\"log\")\n",
    "\n",
    "g.savefig(dir + \"/pairplot.png\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import jaccard_score\n",
    "from scipy.spatial import distance\n",
    "\n",
    "df_g = {}\n",
    "for n in range(N_GROUP):\n",
    "    df_g[n] = df_bins_label[df_bins_label.flag == f'group{n}']\n",
    "\n",
    "mins = min([len(df_g[i]) for i in range(N_GROUP) ])\n",
    "\n",
    "print(f\"Length of {N_GROUP} groups is {[len(df_g[i]) for i in range(N_GROUP) ]}\")\n",
    "print(f\"Mins is: {mins:,.0f}\")\n",
    "\n",
    "df_final = pd.DataFrame(zip(att_col), columns= ['attributes'])\n",
    "df_distance = {}\n",
    "for i in range(N_GROUP):\n",
    "    df_distance[i] = {}  # Initialize df_distance[i] as empty dictionary\n",
    "    for y in range(i + 1, N_GROUP):\n",
    "        if i != y and i < y:\n",
    "            df_temp = pd.DataFrame(zip(att_col), columns= ['attributes'])\n",
    "            list_temp = []\n",
    "            for a_col in att_col:\n",
    "                distance_iy = distance.cosine(df_g[i][:mins][a_col].values.flatten(), df_g[y][:mins][a_col].values.flatten())\n",
    "                similar_rate = 1 - distance_iy\n",
    "                list_temp.append(similar_rate)\n",
    "            df_temp[f'similar{i}{y}'] = list_temp    \n",
    "            df_final = df_final.merge(df_temp, on = 'attributes',how ='left')\n",
    "\n",
    "display(df_final.mean())\n",
    "display(df_final.style.background_gradient(cmap='YlOrRd' , vmin = 0, vmax =1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Export result to file\n",
    "df_bins_label.drop(columns= [c for c in df_bins_label.columns if '_bins' in c]).to_excel(dir + \"/result.xlsx\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Lưu file vào dw để Test\n",
    "import pandas as pd\n",
    "\n",
    "dir = r'C:\\Users\\Admin\\OneDrive\\FRT\\6. Stored Data\\AkachainMember\\AB_Testing_230404\\Round 2_230419'\n",
    "df_import = pd.read_excel(dir + \"/result.xlsx\")\n",
    "df_import = df_import[['id', 'customer_key', 'flag']]\n",
    "df_import.to_sql('F_ICT_AKACHAIN_LOYALTY_ABTEST_0423',getEngine(SaleMart),  if_exists='replace', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Kết quả sau khi chạy.\n",
    "# Load list khách vào và so sánh\n",
    "# Những thằng nào là thằng quay lại\n",
    "\n",
    "df_back = pd.read_csv(r'C:\\Users\\Admin\\OneDrive\\FRT\\6. Stored Data\\AkachainMember\\AB_Testing_230404\\Round 2_230419\\List_Final\\List_Cus_Back_export_230508.csv')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_back"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_result = df_cal.merge(df_back, on ='customer_key', how='inner')\n",
    "\n",
    "# Pair plot of distributions\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "pair_col = att_col.copy()\n",
    "pair_col.append('flag')\n",
    "g = sns.pairplot(df_result[pair_col],\n",
    "                 height=1.8, \n",
    "                 corner=False,\n",
    "                 plot_kws={'alpha':0.3},\n",
    "                 hue='flag',\n",
    "                 kind='kde',\n",
    "                )\n",
    "# plt.show()\n",
    "log_columns = [\"total_amt\", \"total_amt_apple\", 'monetary_akc', 'total_buy', ]\n",
    "\n",
    "# for ax in g.axes.flat:\n",
    "#     if ax.get_xlabel() in log_columns:\n",
    "#         ax.set(xscale=\"log\")\n",
    "\n",
    "g.savefig(dir + \"/pairplot_result.png\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Request 07/06/2023: Check bất thường"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "dir =  r'C:\\Users\\Admin\\OneDrive\\FRT\\6. Stored Data\\AkachainMember\\Check_Batthuong_20230607'\n",
    "\n",
    "df = pd.read_excel(dir + \"\\\\AKC_TRANSACTION_20230529.xlsx\")\n",
    "df_tich_tieu = pd.read_csv(dir + '\\\\tich_tieu_sql.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "diem = pd.read_csv(r\"C:\\Users\\Admin\\OneDrive\\FRT\\6. Stored Data\\AkachainMember\\Check_Batthuong_20230607\\tich_tieu.txt\", encoding = 'utf-8', sep = '\\t')\n",
    "diem = diem[~diem['DOCENTRY_MAPPING'].isna()]\n",
    "diem = diem.astype({'DOCENTRY_MAPPING':'int'})\n",
    "diem.drop(columns=['IsGrandTotalRowTotal'], inplace=True)\n",
    "# diem.rename(columns={'Số_điểm_tiêu':'DIE'})\n",
    "diem"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f_date = df[df['TRANSACTION_TYPE'] == 'Sales Invoice'].groupby('INVOICE_HEADER', as_index=False).agg(MIN_DATE_BUY = (\"DATE_KEY\", 'min'))\n",
    "f_date['MIN_DATE_BUY'] = pd.to_datetime(f_date['MIN_DATE_BUY'], format = '%Y%m%d')\n",
    "f_date"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "group_col = ['MÃ THÀNH VIÊN', 'CUSTOMER_NAME', 'CUSTOMER_PHONE', 'CUSTOMER_KEY','TRANSACTION_TYPE','INVOICE_HEADER']\n",
    "df_g = df.groupby(group_col, as_index=False).agg(\n",
    "    totalAmount = ('LIST_AMT_AF_VAT', 'sum'),\n",
    "    totalProduct = ('PRODUCT_NAME', 'nunique'),\n",
    "    totalCategory = ('PRODUCT_CATEGORY_NAME', 'nunique'),\n",
    "    totalCategory_level_2 = ('PRODUCT_GROUP_NAME', 'nunique'),\n",
    ")\n",
    "\n",
    "df_product_name = df.groupby(group_col)['PRODUCT_NAME'].apply(list).reset_index(name='PRODUCT_NAME')\n",
    "df_g = df_g.merge(df_product_name, on=group_col)\n",
    "\n",
    "df_g['PRODUCT_NAME'] = df_g['PRODUCT_NAME'].apply(lambda x: '\\n'.join(x) if isinstance(x, list) else x)\n",
    "\n",
    "\n",
    "# df_g.to_excel(dir + \"/result.xlsx\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_final = df_g.merge(diem, left_on= 'INVOICE_HEADER', right_on='DOCENTRY_MAPPING', how='left').drop(columns = ['DOCENTRY_MAPPING'])\n",
    "df_final = df_final.merge(f_date, on ='INVOICE_HEADER', how = 'left')\n",
    "# df_final = df_final[df_final['TRANSACTION_TYPE'] == 'Sales Invoice']\n",
    "# df_final['Số_điểm_tiêu'] = np.where(df_final['TRANSACTION_TYPE'] != 'Sales Invoice', 0 , df_final['Số_điểm_tiêu'])\n",
    "# df_final['Số_điểm_tích_lũy'] = np.where(df_final['TRANSACTION_TYPE'] != 'Sales Invoice', 0 , df_final['Số_điểm_tích_lũy'])\n",
    "# df_final\n",
    "# df_final = df_final.sort_values(by = ['MÃ THÀNH VIÊN', 'MIN_DATE_BUY'], ascending = [True,True]) \n",
    "# df_final.to_excel(dir + \"/result.xlsx\", index = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_final['Số_điểm_tiêu'].sum(), df_final['Số_điểm_tích_lũy'].sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_tich_tieu = pd.read_csv(dir + '\\\\tich_tieu_sql.csv')\n",
    "df_tich_tieu['type'] = np.where(df_tich_tieu['Lý do'].str.contains('Invoice'), 'Tich',\n",
    "                                np.where\n",
    "\n",
    "df_tich_tieu[df_tich_tieu['type'] == 1]['Lý do'].unique()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  },
  "vscode": {
   "interpreter": {
    "hash": "0d591c6e422414675974e227c13f5382000c440fedd3c5006ef2be5d887f0ba7"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
